{
	"nodes":[
		{"id":"timeline","type":"text","text":"# CSCI 6751 Artificial Intelligence\n\n**Timeline: January 14 → February 5, 2026 (4 weeks)**\n\nLeft to Right: Lecture01 → Week02 → Week03 → Week04","x":-400,"y":-300,"width":600,"height":160,"color":"6"},
		{"id":"lecture01","type":"text","text":"## Lecture 01 (Jan 14)\n**Course Introduction & AI Foundations**","x":-1200,"y":0,"width":300,"height":100,"color":"1"},
		{"id":"lecture01-ai-def","type":"text","text":"### AI Definition\n- Learning\n- Reasoning\n- Problem Solving\n- Perception\n- Language Understanding","x":-1200,"y":150,"width":300,"height":200,"color":"3"},
		{"id":"lecture01-history","type":"text","text":"### AI History\n- 1950: Turing Test\n- 1956: Dartmouth Conference\n- 1980s: Expert Systems\n- 1980s-90s: Neural Networks\n- 2012: Deep Learning Breakthrough\n- 2020+: GPT-3/4, ChatGPT","x":-1200,"y":400,"width":300,"height":250,"color":"4"},
		{"id":"lecture01-expert-systems","type":"text","text":"### Expert Systems\n- Knowledge Base\n- Inference Engine\n- Example: MYCIN (medical diagnosis)\n- Limitation: No learning capability","x":-850,"y":150,"width":300,"height":200,"color":"4"},
		{"id":"lecture01-nn","type":"text","text":"### Neural Networks\n- Inspired by biological neurons\n- Multi-layer structure: Input → Hidden → Output\n- Requires GPU (NVIDIA)\n- Key figures: Geoffrey Hinton, Yann LeCun","x":-850,"y":400,"width":300,"height":200,"color":"2"},
		{"id":"week02","type":"text","text":"## Week 02\n**Machine Learning Fundamentals**","x":-450,"y":0,"width":300,"height":100,"color":"1"},
		{"id":"week02-ml-def","type":"text","text":"### Machine Learning Definition\n\n**T-P-E Framework:**\n- T (Task): What to accomplish\n- P (Performance): How well to do it\n- E (Experience): Data/training\n\nML focuses on prediction;\nStatistics focuses on explanation.","x":-450,"y":150,"width":300,"height":220,"color":"3"},
		{"id":"week02-supervised","type":"text","text":"### Supervised Learning\nLabeled data required.\n\n**Two categories:**\n\n1. **Regression** → Continuous output\n   - Example: House price prediction\n\n2. **Classification** → Discrete categories\n   - Example: Spam detection","x":-450,"y":420,"width":300,"height":250,"color":"2"},
		{"id":"week02-unsupervised","type":"text","text":"### Unsupervised Learning\nNo labeled data.\n\n**Two categories:**\n\n1. **Clustering** → Grouping\n   - Example: Customer segmentation\n\n2. **Dimensionality Reduction**\n   - Example: PCA","x":-450,"y":720,"width":300,"height":230,"color":"2"},
		{"id":"week02-rl","type":"text","text":"### Reinforcement Learning\nTrial & Error + Reward/Penalty\n\n**Components:**\n- States (S)\n- Actions (A)\n- Rewards (R)\n- Policy\n\nExamples: Game AI, Robot navigation","x":-450,"y":1000,"width":300,"height":250,"color":"2"},
		{"id":"week02-encoding","type":"text","text":"### Data Encoding\nCategorical → Numeric\n\n**Label Encoding** (avoid for features)\n- Toyota→1, Honda→2 (implies false ordering)\n\n**One-Hot Encoding** (for tree models)\n- Red→[1,0,0], Green→[0,1,0]\n\n**Dummy Encoding** (for linear models)\n- K categories → K-1 columns (avoids multicollinearity)","x":-100,"y":420,"width":300,"height":330,"color":"5"},
		{"id":"week03","type":"text","text":"## Week 03\n**Linear Regression**","x":300,"y":0,"width":300,"height":100,"color":"1"},
		{"id":"week03-regression-vs-classification","type":"text","text":"### Regression vs Classification\nKey distinction: Output type\n\n**Continuous values** → Regression\n- Example: Hospital stay (4.6 days)\n\n**Discrete categories** → Classification\n- Example: Disease type (1/2/3)\n\nNote: Must be able to distinguish.","x":300,"y":150,"width":300,"height":240,"color":"6"},
		{"id":"week03-normal-equation","type":"text","text":"### Normal Equation\nClosed-form solution.\n\n**Formula:**\nθ = (X^T X)^(-1) X^T y\n\n**Requirements:**\n- Matrix transpose\n- Matrix inverse\n- Determinant calculation\n\nAdvantage: One-step solution\nDrawback: Computationally expensive","x":300,"y":440,"width":300,"height":280,"color":"5"},
		{"id":"week03-gradient-descent","type":"text","text":"### Gradient Descent\nIterative optimization.\n\n**Update rule:**\nθ := θ - α ∇J(θ)\n\n**Components:**\n- α = Learning Rate\n- ∇J = Gradient (negative direction)\n\n**Variants:**\n- Batch GD: All data\n- SGD: Single sample\n- Mini-batch: Small batches","x":300,"y":770,"width":300,"height":300,"color":"5"},
		{"id":"week03-learning-rate","type":"text","text":"### Learning Rate (α)\nTrade-off consideration:\n\n- α too large → Overshoot\n- α too small → Slow convergence\n\nNote: α is a hyperparameter.","x":650,"y":770,"width":280,"height":200,"color":"3"},
		{"id":"week04","type":"text","text":"## Week 04 (Feb 5)\n**Overfitting & Regularization**","x":1000,"y":0,"width":300,"height":100,"color":"1"},
		{"id":"week04-overfitting","type":"text","text":"### Overfitting\nModel too complex; memorizes noise.\n\n**Symptoms:**\n- Low training error\n- High test error\n\n**Causes:**\n- High polynomial degree\n- Too many parameters","x":1000,"y":150,"width":300,"height":230,"color":"6"},
		{"id":"week04-underfitting","type":"text","text":"### Underfitting\nModel too simple; fails to capture patterns.\n\n**Symptoms:**\n- High training error\n- High test error\n\n**Cause:**\nModel complexity insufficient (e.g., linear fit for curved data)","x":1000,"y":430,"width":300,"height":220,"color":"6"},
		{"id":"week04-evaluation","type":"text","text":"### Evaluation Criteria\nCompare training vs test error.\n\n| Train Error | Test Error | Diagnosis |\n|-------------|------------|----------|\n| High | High | Underfitting |\n| Low | Low | Good fit |\n| Very Low | High | Overfitting |\n\nKey insight:\n\"We don't care only about training error. Test error is what matters.\"","x":1000,"y":700,"width":300,"height":330,"color":"6"},
		{"id":"week04-regularization","type":"text","text":"### Regularization\nPrevents overfitting via penalty terms.\n\n**Principle:**\nminimize: MSE + λ × (penalty)\n\n**Two types:**\n\n1. **L2 (Ridge)**: Penalty on Σw²\n   - Shrinks coefficients (non-zero)\n\n2. **L1 (Lasso)**: Penalty on Σ|w|\n   - Drives some coefficients to zero (feature selection)","x":1370,"y":150,"width":320,"height":310,"color":"5"},
		{"id":"week04-lambda","type":"text","text":"### Lambda (λ)\nRegularization strength.\n\n- Large λ → Simpler model → Potential underfitting\n- Small λ → Complex model → Potential overfitting\n- λ = 0 → No regularization\n\nNote: λ is a hyperparameter.\n\n**Selection method:**\nGrid Search + Cross-Validation","x":1370,"y":510,"width":320,"height":300,"color":"3"},
		{"id":"week04-train-test-split","type":"text","text":"### Train-Test Split\nData partitioning.\n\n- Train Set (80%): Model training\n- Test Set (20%): Generalization evaluation\n\nCritical rule: Never train on test set.\n\n```python\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n```","x":1370,"y":860,"width":320,"height":330,"color":"4"},
		{"id":"hyperparameters","type":"text","text":"## Hyperparameters\nManually set parameters.\n\n1. Learning Rate (α) - Week03/04\n2. Polynomial Degree - Week04\n3. Lambda (λ) - Week04\n\n**Characteristics:**\n- Not learned from data\n- Selected via Cross-Validation\n- Significant impact on model performance","x":650,"y":1100,"width":350,"height":280,"color":"1"},
		{"id":"key-formulas","type":"text","text":"## Key Formulas\nEssential for exams.\n\n1. **Normal Equation:**\n   θ = (X^T X)^(-1) X^T y\n\n2. **Gradient Descent:**\n   θ := θ - α ∇J(θ)\n\n3. **Linear Regression Gradient:**\n   ∇J(θ) = (1/m) X^T (Xθ - y)\n\n4. **Ridge (L2):**\n   minimize: Σ(y-ŷ)² + λΣw²\n\n5. **Lasso (L1):**\n   minimize: Σ(y-ŷ)² + λΣ|w|","x":-100,"y":1100,"width":380,"height":360,"color":"5"},
		{"id":"exam-tips","type":"text","text":"## Exam Focus\nKey points emphasized by professor.\n\n1. **Distinguish Regression vs Classification**\n   - Examine output type\n\n2. **Memorize Normal Equation**\n   - (X^T X)^(-1) X^T y\n\n3. **Identify Overfitting vs Underfitting**\n   - Compare training vs test error\n\n4. **Understand Regularization**\n   - L1 vs L2 differences\n   - Lambda's effect on model\n\n5. **Hyperparameter Tuning**\n   - Impact of α, degree, λ\n\nMidterm: February 24, 2026","x":300,"y":1150,"width":350,"height":450,"color":"6"},
		{"id":"next-steps","type":"text","text":"## Upcoming Topics\nProjected future weeks:\n\n- Classification Algorithms\n  - Logistic Regression\n  - Decision Trees\n  - SVM\n- Neural Networks (Deep Dive)\n- CNNs (Computer Vision)\n- RNNs/Transformers (NLP)\n- Unsupervised Learning Algorithms\n- Reinforcement Learning (Practical)\n\nFinal Project: 2-3 person groups","x":1050,"y":1150,"width":350,"height":350,"color":"4"},
		{"id":"ai-hierarchy","type":"text","text":"## AI Hierarchy\nTaxonomy of AI subfields.\n\n```\nArtificial Intelligence\n├── Expert Systems\n├── Fuzzy Computing\n├── Robotics\n├── Natural Language Processing\n└── Machine Learning\n    ├── Traditional ML\n    │   ├── Linear Regression\n    │   ├── Decision Trees\n    │   ├── SVM\n    │   └── Clustering\n    └── Deep Learning\n        ├── Neural Networks\n        ├── CNNs (Vision)\n        ├── RNNs (Sequences)\n        └── Transformers (NLP)\n```\n\nNote: Deep Learning is a subset of ML.","x":-1050,"y":1070,"width":400,"height":480,"color":"4"}
	],
	"edges":[
		{"id":"timeline-lecture01","fromNode":"timeline","fromSide":"bottom","toNode":"lecture01","toSide":"right"},
		{"id":"timeline-week02","fromNode":"timeline","fromSide":"bottom","toNode":"week02","toSide":"top"},
		{"id":"timeline-week03","fromNode":"timeline","fromSide":"bottom","toNode":"week03","toSide":"top"},
		{"id":"timeline-week04","fromNode":"timeline","fromSide":"right","toNode":"week04","toSide":"left"},
		{"id":"lecture01-ai-def-link","fromNode":"lecture01","fromSide":"bottom","toNode":"lecture01-ai-def","toSide":"top"},
		{"id":"lecture01-history-link","fromNode":"lecture01","fromSide":"bottom","toNode":"lecture01-history","toSide":"top"},
		{"id":"lecture01-expert-link","fromNode":"lecture01","fromSide":"bottom","toNode":"lecture01-expert-systems","toSide":"left"},
		{"id":"lecture01-nn-link","fromNode":"lecture01","fromSide":"bottom","toNode":"lecture01-nn","toSide":"top"},
		{"id":"lecture01-to-week02","fromNode":"lecture01-nn","fromSide":"top","toNode":"week02","toSide":"bottom","color":"2"},
		{"id":"week02-ml-def-link","fromNode":"week02","fromSide":"bottom","toNode":"week02-ml-def","toSide":"top"},
		{"id":"week02-supervised-link","fromNode":"week02","fromSide":"bottom","toNode":"week02-supervised","toSide":"top"},
		{"id":"week02-unsupervised-link","fromNode":"week02","fromSide":"bottom","toNode":"week02-unsupervised","toSide":"top"},
		{"id":"week02-rl-link","fromNode":"week02","fromSide":"bottom","toNode":"week02-rl","toSide":"top"},
		{"id":"week02-encoding-link","fromNode":"week02","fromSide":"bottom","toNode":"week02-encoding","toSide":"top"},
		{"id":"week02-to-week03","fromNode":"week02-supervised","fromSide":"right","toNode":"week03","toSide":"bottom","color":"2"},
		{"id":"week03-regression-link","fromNode":"week03","fromSide":"bottom","toNode":"week03-regression-vs-classification","toSide":"top"},
		{"id":"week03-normal-link","fromNode":"week03","fromSide":"bottom","toNode":"week03-normal-equation","toSide":"top"},
		{"id":"week03-gradient-link","fromNode":"week03","fromSide":"bottom","toNode":"week03-gradient-descent","toSide":"top"},
		{"id":"week03-lr-link","fromNode":"week03-gradient-descent","fromSide":"right","toNode":"week03-learning-rate","toSide":"left"},
		{"id":"week03-to-week04","fromNode":"week03-gradient-descent","fromSide":"top","toNode":"week04","toSide":"bottom","color":"2"},
		{"id":"week04-overfitting-link","fromNode":"week04","fromSide":"bottom","toNode":"week04-overfitting","toSide":"top"},
		{"id":"week04-underfitting-link","fromNode":"week04","fromSide":"bottom","toNode":"week04-underfitting","toSide":"top"},
		{"id":"week04-evaluation-link","fromNode":"week04","fromSide":"bottom","toNode":"week04-evaluation","toSide":"top"},
		{"id":"week04-regularization-link","fromNode":"week04","fromSide":"bottom","toNode":"week04-regularization","toSide":"left"},
		{"id":"week04-lambda-link","fromNode":"week04-regularization","fromSide":"bottom","toNode":"week04-lambda","toSide":"top"},
		{"id":"week04-split-link","fromNode":"week04-evaluation","fromSide":"right","toNode":"week04-train-test-split","toSide":"left"},
		{"id":"hyperparameters-lr","fromNode":"week03-learning-rate","fromSide":"bottom","toNode":"hyperparameters","toSide":"top","color":"4"},
		{"id":"hyperparameters-lambda","fromNode":"week04-lambda","fromSide":"left","toNode":"hyperparameters","toSide":"top","color":"4"},
		{"id":"formulas-normal","fromNode":"week03-normal-equation","fromSide":"bottom","toNode":"key-formulas","toSide":"top","color":"5"},
		{"id":"formulas-gradient","fromNode":"week03-gradient-descent","fromSide":"bottom","toNode":"key-formulas","toSide":"top","color":"5"},
		{"id":"formulas-regularization","fromNode":"week04-regularization","fromSide":"left","toNode":"key-formulas","toSide":"right","color":"5"},
		{"id":"exam-overfitting","fromNode":"week04-overfitting","fromSide":"bottom","toNode":"exam-tips","toSide":"top","color":"6"},
		{"id":"exam-regression","fromNode":"week03-regression-vs-classification","fromSide":"bottom","toNode":"exam-tips","toSide":"top","color":"6"},
		{"id":"hierarchy-ml","fromNode":"lecture01-nn","fromSide":"bottom","toNode":"ai-hierarchy","toSide":"top","color":"4"}
	]
}
