# Week 02: Vocabulary and Key Terms

## Core Machine Learning Terms

### **Machine Learning (ML)**
The study of algorithms that improve performance on a given task (T) with experience (E), measured by performance metric (P).

### **Artificial Intelligence (AI)**
Broader field encompassing machine learning, expert systems, robotics, NLP, and fuzzy computing.

### **Deep Learning (DL)**
Subset of machine learning using neural networks with multiple layers; particularly effective for images, speech, and text.

---

## Learning Paradigms

### **Supervised Learning**
Learning from labeled data where each training example has a known target value.

### **Unsupervised Learning**
Learning from unlabeled data to discover hidden structures or patterns.

### **Semi-Supervised Learning**
Learning from a mix of labeled and unlabeled data (e.g., 80% labeled, 20% unlabeled).

### **Reinforcement Learning**
Learning from rewards and penalties by taking actions in an environment and adjusting behavior based on feedback.

---

## Problem Types

### **Regression**
Supervised learning problem where the target variable is a continuous/float number (e.g., price, temperature, score 0-100).

### **Classification**
Supervised learning problem where the target variable is a category or class (e.g., spam/not spam, grades A/B/C).

### **Clustering**
Unsupervised learning technique that groups similar data points together based on feature similarity.

### **Dimensionality Reduction**
Unsupervised technique to reduce the number of features while preserving important information (e.g., PCA - Principal Component Analysis).

---

## Data Components

### **Features / Predictors / Input Variables (X)**
The input characteristics used to make predictions; can be one or multiple variables.

### **Target / Label / Output Variable (Y)**
The value we want to predict; always a single value in standard ML.

### **Training Data**
The dataset used to train the model and learn patterns.

### **Test Data**
Separate dataset used to evaluate model performance on unseen data.

### **Ground Truth**
The actual, correct values for the target variable.

---

## Model Concepts

### **Model / Function**
Mathematical representation learned from data that maps inputs (X) to outputs (Y).

### **Error / Loss**
The difference between model predictions and ground truth; goal is to minimize this.

### **Pattern**
Recurring structures or relationships in data that the model learns to recognize.

### **Prediction**
The output generated by a trained model when given new input data.

### **Generalization**
Model's ability to perform well on new, unseen data (not just training data).

---

## Neural Network Terms

### **Neural Network**
Machine learning model inspired by brain structure, consisting of interconnected layers of nodes.

### **Layer**
A collection of nodes (neurons) in a neural network; networks have input, hidden, and output layers.

### **Deep Belief Net**
A type of deep neural network architecture mentioned in lecture.

### **Edge Detection**
First layer task in image processing neural networks - identifying lines and boundaries.

### **High-Level Features**
Complex patterns learned in deeper layers (e.g., eyes, faces) built from simpler low-level features.

### **Low-Level Features**
Simple patterns detected in early layers (e.g., edges, lines, basic shapes).

---

## Encoding Techniques

### **Encoding**
Converting categorical (text) data into numeric format for machine learning algorithms.

### **Label Encoding** ❌
Assigning sequential numbers to categories (1, 2, 3...); problematic for features as it creates false ordering.

### **One-Hot Encoding** ✅
Creating K binary columns for K categories; each record has 1 in its category column, 0 elsewhere.

### **Dummy Encoding** ✅
Creating K-1 binary columns for K categories; removes one category to avoid multicollinearity.

### **Multicollinearity**
Statistical issue where features are dependent on each other; avoided by using dummy encoding in linear models.

---

## Data Types

### **Numeric Data**
- **Real values**: Continuous numbers (2.5, 100.75, -3.14)
- **Integer values**: Whole numbers (1, 2, 50, 100)

### **Categorical Data**
Discrete classes or groups (colors, car brands, cities).

### **Binary Data**
Special case of categorical with only two values (Yes/No, True/False, 0/1).

### **Tabular Data**
Data organized in rows and columns (like spreadsheets); traditional ML works well on this format.

### **Image Data**
Visual data represented as pixel matrices with RGB values; deep learning excels at processing this.

---

## Reinforcement Learning Terms

### **Agent**
The learner or decision-maker in reinforcement learning (e.g., robot, AI player).

### **Environment**
The world in which the agent operates and interacts.

### **Action (At)**
A move or decision made by the agent at time t.

### **State (St)**
Current situation or condition of the agent at time t.

### **Reward (Rt)**
Feedback signal from environment at time t; positive for good actions, negative for bad.

### **Penalty**
Negative reward signaling that an action was undesirable.

### **Policy**
Strategy or set of rules determining which actions to select to maximize cumulative rewards.

---

## Specific Algorithms & Techniques

### **Linear Regression**
Regression algorithm that fits a linear function to data; uses dummy encoding to avoid multicollinearity.

### **Decision Trees**
Tree-based algorithm for classification/regression; uses one-hot encoding; doesn't require feature scaling.

### **SVM (Support Vector Machine)**
Classification algorithm that finds optimal decision boundaries.

### **PCA (Principal Component Analysis)**
Dimensionality reduction technique that finds principal components capturing most variance.

### **Logistic Regression**
Classification algorithm (despite "regression" in name); uses dummy encoding.

---

## Application Domains

### **Speech Recognition**
Converting spoken language to text using ML models that analyze frequency patterns.

### **Object Detection**
Identifying and locating objects in images (used in autonomous vehicles, security).

### **Face Recognition**
Identifying or verifying individuals from facial features.

### **Market Segmentation**
Grouping customers based on behavior, preferences, or characteristics.

### **Social Network Analysis**
Analyzing connections and patterns in social graphs.

### **Autonomous Vehicles / Self-Driving Cars**
Vehicles using ML for navigation, object detection, and decision-making.

### **Medical Diagnosis**
Using ML to predict diseases or classify medical conditions from patient data.

---

## Image Processing Terms

### **Pixel**
Single point in an image; the smallest unit.

### **RGB Values**
Red, Green, Blue color channels; each ranges from 0-255.

### **Image Matrix**
Numerical representation of an image (e.g., 60x60x3 for 60x60 image with 3 color channels).

### **Sepal**
Outer part of a flower (used in Iris dataset example).

### **Petal**
Inner colored part of a flower (used in Iris dataset example).

---

## Statistical Concepts

### **Average / Mean**
Sum of values divided by count; basic statistical measure used in ML.

### **Standard Deviation**
Measure of data spread; important for understanding data distribution.

### **Relationship / Correlation**
Statistical connection between variables; statistics finds these, ML uses them for prediction.

### **Significance**
Statistical measure of whether a relationship is meaningful or due to chance.

---

## Practical Examples from Lecture

### **Iris Dataset**
Classic ML dataset with 150 flower samples; 4 features (sepal/petal dimensions); 3 species classes.

### **Arctic Sea Ice Level**
Regression example predicting ice coverage over time (1980-present).

### **Tumor Classification**
Medical classification example: malignant vs benign based on tumor size.

### **Auto Insurance Premium**
Regression example with mixed numeric and categorical features (age, car model, experience).

### **Mars Navigation**
Example where human expertise doesn't exist; ML trains on Mars images.

### **House Price Prediction**
Regression with features like square footage, bedrooms, location.

---

## Acronyms

- **AI**: Artificial Intelligence
- **ML**: Machine Learning
- **DL**: Deep Learning
- **NLP**: Natural Language Processing
- **PCA**: Principal Component Analysis
- **SVM**: Support Vector Machine
- **RGB**: Red, Green, Blue
- **CNN**: Convolutional Neural Network
- **RNN**: Recurrent Neural Network

---

## Important Distinctions

| **Statistics** | **Machine Learning** |
|---|---|
| Find relationships | Make predictions |
| Interpret connections | Optimize accuracy |
| "Does X cause Y?" | "Given X, predict Y" |
| Foundation/basis | Higher-level application |

| **Regression** | **Classification** |
|---|---|
| Continuous output | Categorical output |
| Float numbers | Discrete classes |
| Example: $100,000 | Example: A/B/C grade |

| **Supervised** | **Unsupervised** |
|---|---|
| Labeled data | Unlabeled data |
| Predict target | Find structure |
| Needs Y values | Only X features |

| **One-Hot Encoding** | **Dummy Encoding** |
|---|---|
| K columns for K categories | K-1 columns |
| No multicollinearity issue avoidance | Avoids multicollinearity |
| Use for tree/neural models | Use for linear models |

---

## Key Numbers Mentioned

- **150**: Number of samples in Iris dataset (50 per species)
- **4**: Number of features in Iris dataset
- **3**: Number of species in Iris dataset
- **0-255**: Range for RGB color values
- **60x60**: Example image dimensions mentioned
- **80%-20%**: Example split for semi-supervised learning
- **20,000**: Example number of records in a dataset
- **1,000**: Example number of features before dimensionality reduction
