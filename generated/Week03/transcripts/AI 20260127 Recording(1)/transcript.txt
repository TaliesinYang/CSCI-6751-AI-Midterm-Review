Do you have any idea about how you can find the universal matrix? Remember from my school? You need to find the determinant, right? So I can bring one example. You see, for example, 11 by 4. Okay? How this one is got from this one? That's the question, okay? Let me explain here. So you can just review on Google how to find the inverse of matrix, but in general it's very easy. So what you need to do, so you want to find the inverse for corresponding to this one, right? Which is 11 by 4. How you can find? Just review all these lines. Wherever 4 is located there, its row and columns should be written. What is left? 6, 7, 7, 10. And multiply 6 by 10 is 16. Minus 49. What is left? 11. It's going to be 11 divided by... Let's go to this one. How I got this? This is corresponding to this 4. And this 4 is in first row, second column. You remove that one, you're going to have only 4 and 7 and 6. 40 minus 42. So yeah, okay. I'm looking for... I'll explain how we get this value. So where is this value? Its corresponding value is 4. Where is 4 located? First row, second column. Remove all the values in the first row and second column. What is left? The matrix of 4, 7, 6, 10. Then the determination of that one is going to be like this. Multiply this, minus this one. 4 times 10. 40 minus 42. Which means minus 2. So we have minus, let's say, minus 2 here. I can explain what it's getting. Minus 2, and what is the value for 4 times? So... And then, again, here. What is getting positive? For the odd values, for this one, it's going to positive. For the next one, you multiply that one with one extra minus. It's giving you that how to calculate the immersive matrix. So you want to get this 6. Again, remove this row, remove this column. What is left? 27 minus 36. Which is going to be... Sorry, 24 minus... Minus 8. 28 minus 36, which is 8. And it could be 8.6. Just check this one. Maybe this calculation here is not correct. Okay? So what you need to do, just take this matrix and... And learn how you can find the inverse of that. By yourself. If there is any miscalculation, you can give this one to... Yeah, to AI and ask that what would be the... Okay, do that one. Just provide this matrix to AI and ask that what would be the inverse. Is it the same thing or is it something different? You can check on your side and correct it if it's not correct. And what is important to learn that if you have a matrix, how to get the inverse? Because everything depends on this minus. With this assumption that inverse is this, output is going to be... Just check this one on your side. That's it. It should be, I believe, some revision of this. But you can check. Same thing. Here, you see this is an example. When you have 3 by 3, you need to get inverse of result. Yeah. Yeah, just give you that how you can find the inverse of the matrix. Any questions? Will this kind of question show in our midterm? The calculation, I mean. In our midterm, we want to have... We might have something like this. And ask you what would be the optimal parameters. Yeah. Is it difficult to hear? Which one is and how we get this value? Can we use AI? Can we use Python? You can use Python, but in the exam, you don't have the Python, right? Yeah. You want to understand how we get this value. But you only give us the data. We have to assume they only have one or two degrees. What I want to give you, for example, I'm going to provide something like this. X1 and Y, or X1 and X2 and Y. And ask you that suppose we have a linear function. I give you that assumption. Okay. That makes sense. Just with that assumption, you're going to have to... So we have to remember the formula. This formula should be, yeah, it's a good point. You need to memorize this formula. That's it. It is a closed form. I also memorize that. Okay? X transpose, easy. X transpose X inverse. X transpose Y. That's it. If you know this one, all the linear regulations can be solved by this. Even the simple one, multivariable, or polynomial, any data. Okay. The takeaway. What is the conclusion here? What is the conclusion? Okay. People at the back. Let me know what is the conclusion. What you learned today. Let's give you that. Numbers. I didn't learn much. I didn't discuss too much. Numbers was just a part of our discussion. What was the A? Find the parameter. Find the parameter for what? Linear regression. Okay. Okay. Let's summarize together. The idea here is that suppose that you have some points. Okay? Some points. Like I have one point here. For that point, I have its X and I have its Y. I have lots of points like this. And I want to approximate or feed a linear function. The best linear function is that the one that which has the minimum error. Means that after you feed that function, the difference between that point. And that's why all the points, if you sum up together, should be a small number. The bigger number means that your feed is not good. And what you need to do? Increase the linearity to polynomial. They need to. Okay? So. And then, see that what would be the optimal degree to? The same if it's not good, increase to degree three. Okay? This is the general idea of philosophy behind the linear regression. The only point is that how we can find the coefficients for that line or polynomial. Find that line. We just need to memorize this. You know this one? You can bring everything in this format. And this is our inputs. And this is our outputs. And this is the polynomials, that's like coefficients. The best coefficient for the polynomial. Of course, if this polynomial is linear, you're going to have this coefficient. If you add more sentences to the polynomial, you're going to have more coefficients. Okay? That's our long story short. What we discussed. Okay? And what was the name of this method? This square method or closed form. Closed form. Okay? Closed form. This square solution. Okay. I have some questions for you. Okay. So, one question here is that. What is that? This is my x. My x. My x. And this is my y. Okay? And I have some points here. One point here, one point here, one point here, one point here, one point here. And one point here. The other point here. And the other point here. Okay. And maybe one point here, one point here, one point here. Okay. So what do you suggest as the best function for it? This goes step by step. What is the initial method? First method. Simulinear. Linear. Or simple. Okay. Linear function. And suppose that I pick the linear function and it is, or linear function of maybe one, going to be something like this. Okay. This assumption. Okay. Or maybe later. Yeah. What is the issue here? The one point is not fit. We have some errors here. Okay. It shows that it's not fit there. Okay. What do you suggest? Increase the data. Degree. Okay. Suppose that rather than this one, I have, I have this one. Like the back you can see. Okay. So suppose that I have something like this one. Okay. Maybe I can say that. Okay. Suppose that I have a polynomial of higher degree, something like this, which tries to cover every point. Every single point. Okay. And it's degree is something of degree of five. Okay. For the red, for the green one, degree is five. For the red one, degree is. Okay. What is the issue potential? So right now I don't have any error. You see any error? No. The color of the, of the point. I don't have any error. Is it good? Okay. Who says yes, it's good? Who says yes, it's good fit? You understand my question? My question is that, is this green, it is good? At red it's good. Because I, it fits all the points. Okay. And suppose that you have 85. And 85 is something like this. Alpha zero plus alpha one X one and I covered this one. So what is the potential issue? Okay. What happens for new points? So like the idea here is that, and I'm looking at the points here, that's like in general the type of relationship is linear. What I understand is that some points here, like this one and this one, those are kind of like noise. But in general, I have this relationship. It's a linear thing. Maybe there are some outliers, some points, some, some I don't know, noises. Okay. This point and this point is noise. We ask our model to learn the noise. Learn the noise. What can be the issue? The issue is that next time, if you had a point here, you have a point here. So this X you want to know you don't have this point in your data. Okay. And you learn this function. Next time somebody asks you, okay, what is the C qualifier? What you do? You just go here and this maps you. Okay. But in reality it should be, it should be some, some how around this one. So what was the issue? The issue comes from here that your model try to learn the noise. When you learn the noise for the new points, maybe the model will make a mistake. Okay. This is called over, you have a point here. All of them could be problematic. Okay. That's the point that somehow increasing the degree should mean the favor of training data. It's good for training data. Training data means originality. So for unseen data, if your model is from the higher degree, maybe for unseen data it makes a mistake. Okay. And this is, this issue is called overfitting. It means performing very well, 100% or throwing down 100% on your original training data. So make a mistake a lot for unseen data. Okay. That's why, like maybe one point you think, okay, let's, let's increase the degree. Then you can, you can eat everything, but it's not good. So what is the good point? Good point or good degree is that like something between. Performs well for training data as well as for just maybe some maybe of three. Okay. That's higher. The maximum degree, but let's not take it higher. The degree higher, of course, if you have some points here, it's going to learn those points as well, but those points are noise of the model. Noise of the model. Don't need to learn those. If you learn those, it's going to make a mistake. Something like this. We have some materials in the class. Okay. Some of the materials are not correct. If you learn those incorrect materials, you're going to make a mistake. Right. So if you see something is, is problematic or errorless. Okay. Something, I guess, somehow similar to this. Don't make it very complicated for the model. Always simple models is the best because that can perform good for your training data and for test data is also okay. That's what we are doing. Finding that optimal balance is something that we're going to talk about that. Make sense? So this is another important point and that is, that is called overfitting. Over, this is one of the most common questions in interviews. If you have a data and it has the overfitting, what will first explain what is the overfitting and the second one is that how we can combat, how we can prevent overfitting. Okay. That's the point that we're going to discuss later. Okay. Any questions? I think that's a good place that we can, we can close the discussion here. I'm going to get back to later. But we learned this least squares solution to the optimal parameters of the regression models. Other method is called gradient descent, which I will discuss about that one. So for now I skipped those, but there was some slides here that I mentioned that I'm going to get back quickly. Let's explain those for five more slides. I think now that they are, that should be very understandable for you. They're talking about, like let's back to this one. You have one point and you want to fit a function. You can just suppose that you have only one point. You have this function, you can consider this function or you can consider this function. Okay. So if you have more points, of course you have a function. With one point, it's not easy to determine which function it is. So this is a degree of two, as I mentioned, because you have some points that you can assume that your model is a plane, linear, or is a degree of two, or is a degree of three. This one is, yeah, this is degree of one and this one is degree of two. Both of them are degree of two actually, but with extra, extra sentences. Yeah, so both of them are degree of two. Okay. So what is the idea? The idea here is that we have our train data. Based on train data, we learn our model. Okay. And now that we know our model, we can pick new features, unseen data, and predict the target arrival based on the function we know. Right? Should be clear, right? Okay. Good. And this is an example of supervised learning. Just here, this is another example here. We want to predict this wise. This is our train data and then for unseen data, we want to predict. Okay. We have something like hyperparameter. I didn't explain yet what is the hyperparameter. I'm going to explain this one next week. So only one slide left. So linear definition for learning problems. Okay. Look at here. So if you want to define or design a learning system, what you need, again, you need to train your learner using some training data and then the testing data. Right? So we have two types of data, training and test data. Okay. So there are different algorithms in machine learning. Okay. And every year we increase those. What is important, first thing is that every machine learning algorithm, sorry, has these three components. Big presentation of the problem, optimization of the parameters, and then evaluate your model. We're going to discuss about details of all of these parts. Right? But in high level, you have a model. That model should work well. How do you do that one? You need to check its efficiency. This module called evaluation. Optimization is finding the best parameters and this presentation means nothing more than forming your problem such a way that you can use for the human algorithm. Let's say, for example, if it's a normal network, your problem should have one representation. If it's a decision tree, for example, if it's a decision tree for categorical data, you don't need encoding. But if it's a normal network, you need to encode this using the one-hot encoding system. That's called representation. And we have different methods here for each one, like, for example, linear regression, decision trees, nearest neighborhood, or k-nearest neighborhood, and also some graphical models like nine phasor, phasor network, and so on and so forth. So, yeah, these are other algorithms. So for evaluation of the model, you have some metrics. We're going to discuss again what is the accuracy, what is precision and recall. For example, if our problem is a regression, we're going to explore it together. But if it's a classification, we require precision and recall. Again, I'll discuss. Yeah, this one shows the loop or pipeline in machine learning. In practice, what we have, we need to understand first the problem itself. We need to gather the data and then train the model, interpret our result, and check the model. If it's not good, get back to first gather more data. Because if we don't have enough data, our model is not perfect. So it shows that what we are doing. Any questions? So these are some slides that I mentioned. Next, where we can go. Now we have everything. The only thing is hyperparameter that I will discuss later. So for now, yeah, up to here, please read it for the next session, especially the topic that I discussed, and learn about these closed modes and also how to calculate the transport. Check this one as well. If there are any errors, just correct this one. That's the end of today's session. If there is no question, we can close here.