So here I want to show an example of how we can apply linear regression in practice. So majorly there is a library for machine learning algorithms and that's called scikit-learn or test-scaler. So what you see here as test-scaler, this is a library in which you can do, you can use the linear regression, later probably neural network and so on and so forth. Decision trees, clustering, everything is there in scikit-learn. So I have imported here a library that's called scikit-learn. So I have imported a library that's called scikit-learn. So I have imported a library that's called scikit-learn. So I have imported a library that's called scikit-learn. So I have imported a library that's called scikit-learn. So I have imported a library that's called scikit-learn. I did get back to this. But the important thing here is that you import the linear regression, which is available in scikit-learn, linear models, OK. And also we have something for train test . Like whatever data set you have, to train And test. They do 80% for train, 20% for test. You train data and then test on test data. the model on there and test it. Okay. So, this Pyplot from Mathplot library is for just plotting, and NumPy is a library for working on tags. Okay? So, the other thing here is that, like, forget about this queue, I'm going to get back to this one. So, first thing here is that I want to create a synthetic data set, which has the 1000 data points, okay? And the formula or equation is something like this, 4, or in section, right? So, 3, and plus numbers. Okay? So, x is random points, 1000 random points, and I multiply that by 3. Okay? So, I'm going to get the values, and then y is coming from this. I added some noises because I want to see some noises in my data. And then if I create a function, I see that, okay, does my model learn those noises or not? Okay? And then here, just plotting, just plotting tags using the tlt.scatter, that means that I'm going to have something like this. Okay? And I'm going to get a little bit smaller. So, what you see here, the x is something with the, because, you see this rand? The output for rand is going to be between 0 and 1, as always, right? Rand, rand, rand of 1. If you want to convert that one to something between 0 to 2, you need to multiply that one by 2. Okay? If you multiply by 5, it's going to be between 0 and 1. So, that's why you see here, x is between 0 and 2. And then between 0 and 2, and your y is coming from this equation. Okay, again, noise is, you see that 4 plus 3x plus our noise, which is a small number, right? Okay, so what I see here is that 3 plus 4x. Maybe I can reduce this to, for example, 500. If I do that, 500, 500, and something like this. Okay, so I see that the relationship is kind of, like, linear. I believe here I can make, let me just show this in a color. Okay, marker, let me see if I can get a marker here, marker. Marker equal, yeah, okay, much, much better. Okay, so, and even we can decrease the size of this. Okay, so I have 1,000 data points, 1,000 data points here. Okay, and I see that this should be something like the relationship, like, linear. But there are a couple of points here, which is half light. Okay, so can I add the random seed? The responsibility of random seed is that if I deactivate this star, each time, these random points which are generated is going to be coming from different distributions. Maybe I run this one, and next time if I run the same thing, the same code, the data point can change. If I want to have reproducibility. This means that each time I have the same data, like, I write the code at home, then I want to show, I want to have the same data point. So, that's why I need to determine my data seed, okay? So, let me say seed 42 or whatever, seed 10, next time that I'm running the code, this data point, random data point will be the same, okay? They are random. But, let's say that it's reproducible, random data point, okay? So, that's why it's necessary to have this one, especially when you want to show an experiment. Okay, so this is something that I have in my data. So, if I print data right now, so data, data, if I print data, okay, so I have X, so if I print X, so you see that X is an array, right, an array of 1,000 data points, and Y is also an array of 1,000 data points, okay? So, now I want to feed the linear regression. What are these steps? These steps here, first of all, I need to add the linear regression here, okay? And then, please think about it. I don't want to do this. I don't want to do this. I don't want to do this. I don't want to do this. I don't want to do this. So, what I'm going to require here is this one. Link a psychic plane. I define a new object of that class, linear regression, and I call that one as link link, okay? So, link link. And then, that object has a function, which is called feed. If I write that one, dot feed, and pass X and Y, it's going to feed a linear regression model. Based on YX, the first parameter is the X. The second parameter is the Y, right? So, and after that one, like if I print, so, for example, I deactivate this bar. So, I print it, that linear regression, the learned linear regression, linear regression, its interception and coefficient. Okay, interception, so, which is this one, and the coefficient, which is this one, okay? Okay. And then, I activate this bar. So, the interception is 4, that means that 4X plus almost 3, right? And that's what we want to, 4 plus 3X. Yeah, 4 plus 3X. And this one is almost 4 plus 3X. It's not exactly the same thing. It's not exactly the same thing because we have some noises here and the model and those noises. That's why it's not exactly 4 plus 3X. Okay? So, I print the function and plot that. So, to plot that one, what I require is, because right now I trained it linear regression, if I plot any X to this and apply credit X, it's going to return the prediction value, right, that Y has here. And I just printed X and it was predicted by X. After this training, X can be unseen data, new data, okay? But for now, just print those and you see that I ask, okay, plot this and the color is going to be red and call that one as linear regression. Okay? So, linear regression, it's cited as my title and if I believe, you know, this and you want to see that later. Okay. Okay. So, the question here is that this is a linear regression. What if I want to have different degrees? If you have none, right? So, or whatever. In that case. So, in that case, you can use the polynomial from psychic plane processing for polynomial features. Okay? So, and if you do that one, you want to create a polynomial, right? So, because I want to show that over here. Okay? So, after importing, create a new object, polynomial features, and give it degrees. And call that one as poly. Now, you say that poly.is. Okay? So, I'm defining a new function, which is polynomial degree 10 to fit x. Okay? And then, I put in my data to train and test. And call that one as 20% as test. And the rest is going to be. Training. Okay? And this is the values that I have. X train, x test. Y train, and y test. Okay? So, and then, after that, I use x train and x test. To fitting the model. And then, as previous one, let's plug that. Okay? So, you see that there is a function of w. You see that it's going to be up and down. Okay? Okay? Oh, if you have 10. Okay, let's see if I have w of 10. Okay. So, what I did. I. Uh. Split my data, right, x, into train and test, and then fit on the, on the train data. Later, I predicted, predicted the values on the, on the, on train data and test data. Okay? So, I want to calculate what is the error of the model. So, one of the matrix that I explained was. Okay. That, it S squared error, right? Y minus, y salary hat is squared, right? For the error of the model. Y minus y hat is squared. If you divide that one by 10, or it shows they. Uh. So you calculate the mean square error here, it's going to show you that how much is the error of the model? So, again from .. The, the , uh... What we're using, the error. Uh. Uh. Cyluit, u dot. Two nou let F. that I mentioned before not quite be used now. So I can do a dot matrix, import mean squared error, okay. And then if you say mean squared error, you pass the prediction of the ground truth, and this one is the prediction of the model on test data. On train data, this is the prediction by the model. This is the ground truth, this is the prediction by the model, and same thing for testing, okay. So it shows that the train MSE is almost 1, and test MSE is less than that, okay. Do we have any sort of like overfitting here? Do you see any overfitting? No, right? Train is 1, and test is less than that. It's not there. Overfitting. Let's increase the degree to higher degrees. Because the data, I believe, is complicated data, probably that's why we don't see that. Okay. So now, again, like test is better than train 100, to see that it can be significant. Yeah, okay. Like when it's 100, like the model is learning more noises, but it's still, for test data, it's close, right? Okay. So I mean that's like in this example, even if that's a higher degree, but still train and test are close to each other. Okay. So if you want to add the regularization, right, so those regularizations, that's what I mentioned, okay. So what we can do, we can call the reach. Okay. So we have region also here, and create a reach, like a model like it from class reach, and then I pass it a string to reach, and then another model based on the losso, right, and pass it a range string to losso. And then if I run the model, okay, so find the reach from there, and okay, so you see that the, this is the trained model, just for 100 samples, okay, and when I add the reach or lasso, it makes it smooth. Our model, even if it's a kind of like higher degree, but it's still smooth. Okay, just visually we can see that we don't have that much, I mean, offset down here. For this one, maybe why we don't see too much offset down, the reason is that we just showed the 100. We show all the data, it's going to show better than those offsets. Okay, you can change this 100 to, for example, let's see if I change that one to 200, that's offset. Offset down is going to be more. You can test this by saying here, you see that's, like, usually you can change those ups and downs. Anyway, like, when we add the reach and regression, it's getting, in other words, like, if you learn the model, which is, has the higher degree, but still, it's not too much ups and downs. Right, it controls those coefficients, coefficients using the reach model. Okay, so, I mean, like, so, we don't need to think about this formula or whatever, but in the background, all of this is done. Right, when you say reach, that means that, and then, like, you create a model based on reach, and then you fit, it means that automatically adds those L2. So, we going to say, like, 8. 6. Right? One pair offen 쓰고, yeah. Alright. Okay. Awesome. That's right. Awesome. value. So default value is 1. You can increase or decrease whatever you want. We discussed about it getting bigger. So same thing for lesson. Here is 1, here is 0.1. You can play with those and see that how the error of the model is increasing or increasing. By doing that, when you see that train and test is getting exactly same. They are almost same values. But here, we had difference. The train was around 1, this one was 0.88. So I added ridge and muscle after that, RMSE for test using ridge. This one just shows for the test, he can print it. It's better to print train data as well to see how much they are going. This one just shows the OK. So ridge. You see there is some difference, but you can control that. We need to check this alpha of the train. The alpha and CDET are very close to each other. But we don't see too much over feeding. The reason is that. The reason is that after applying the ridge or so you don't see that. What kind of change? The only change here is that train is 99.88. Not too much change. Right? Not too much change. Almost same. So you can play with this hyperparameter. And it makes probably the update to hyperparameter and CF update. You can change. OK. I think there is an example here. So take this example. This is your class activity. Maybe for next time, go based on this activity and complete that one. And then we want to have that one in the class. So this is your assignment for next week. There are some requests here. Change the polynomial and observe is there any under feeding or over feeding. OK. And adjust this to 10 and again see that what's happening. OK. And answer this question. Why does regularization include generalization with 5 degree models and so on and so forth. Even you can work on this to see that you can see the over feeding better than this. Maybe increase the number of points. Or make the model complicated. Complicated than this. OK. To see that if there are different values they are getting too much difference. So yeah. Like this is an example that what we discussed. In theory how it's applied in practice. So of course we're updating those finding those coefficients. A, B and this star. We require the optimization. And that optimization which we call that one as a gradient descent is happening inside of this function. OK. Automatically let's see here. There's a linear regression. You can automatically find those coefficients using one of these optimization methods. OK. So if I go to open tab. So here what else we have. Let me change the optimizer. It tells you that what kind of information you can get. Here. Designation. Oh OK. So you see that based on the least square. Yeah. So based on the least square error. This is ordinary least square. Right? Based on the gradient descent. OK. So in background it's happening with multiplication of the matrix and find the optimal coefficient. That's why like when you print the coefficients so it shows you the optimal coefficient. OK. Then linear or then also you can print also. You can print here coefficients of the model. If I apply similar thing. OK. Let's call that one linear. What's mean is linear only. Linear only. Yeah. You see that one. But because it's a degree of 10. So we're gonna have 10 coefficients. 1, 2, 3. Yeah. OK. Those coefficients. So we can print it. And if you change the polynomial degree to more than degree. So your coefficient is gonna be higher. Any question? OK. So. Professor. Yes. How will you text in the midterm? How will you text in the midterm about this? I mean how we will text the knowledge in the midterm? In the midterm? Yeah. Yeah. I think in the exam. Because we don't write in the call during the exam, right? So it's the like MCQ? Multiple. Oh, we're gonna talk about that. We're gonna talk about that. So for next week, complete this assignment. And we don't need to solve it. And like to complete and opening your machine, right? And Google it. And Google call that. Or any ID that you have. And then we're gonna ask you one of, maybe one, two. It's gonna be random. OK. You need to be ready for that. Clear? Good. And find out like the reason. Like check the that you can see any clear overfitting there in the data. Increasing number of ones. Or reduce the number of ones, probably. I think that's the all the material I want to discuss today. OK. Any question? I think that's the end of the session. It's just that it's random and different. I changed some parameters. I don't know if I should say it.