Okay, for example here, we have a couple of points, this is our x and this is our y. Can we approximate a function like this, a linear function, or maybe a non-linear function like this? And the answer was yes, we can do, and we can use the linear regression model to do this. And then, like as you see, this one is not linear, but it's a polynomial as well. We can consider that one as a degree of 2 or something. And again, like apply the version of the linear regression, which is called, we call that one as polynomial. Okay, so, yeah, polynomial regression. So, because you see that here, we have the degree of 2. Still, this function is, I mean, if it is with respect to x, it's non-linear, but we can consider that one as a linear function as well, with respect to our coefficients. So, then we discussed that. If we have a couple of points, how we can determine our function, linear function here, which means that our slope and our intersection, right? So, and there was some discussion around here, and the method which is used called the least squared method, okay? So, it was nothing more than, so, and then what we had was something like this, right? So, using this equation, we can find what is our coefficients, right? Theta 0, Theta 1, or Theta 2, okay? So, it depends on the, if it's a kind of just one variable, or we have more than one variable, one variable. So, what we require here, for example, if it is our points, this is our input, this one is our output, we can, let's say, format our data in something like this. Okay. So, we have one matrix for our outputs, you see, like for our outputs, one matrix for our inputs, we have four points here, so, that's why here you see one, two, three, four points here. So, but, the first one, you see, like, we have one and one, then one and two, one and one, one and two, two and two, and zero and one. The first column. It's going to be, always one, because that one is for beta zero, right? Because this one is beta zero times one. That's why we, we put values, like, one here. Okay. But the other one is beta one times X1, and what is X1, one, one, two, zero. Okay. So, that's the equation behind why we, we put ones over here. So, because beta zero times, it's not X1. It's not x, just y, right? So, and then we have e, beta here. So, what we require is to find the x transpose x, and then the inverse of that one, and multiply by x transpose y. And then we can do something with this example and give them some values for other coefficients. In other words, we determine the function. Okay? So, this method is called this is good. Close format, close form solution. Okay? So, the other method, which is used, this one is like has some limitation. What is the limitation of this method? The limitation is that some of the methods are not inversible. You can't find the inverse of any method. Okay? That's why, like, if you want to apply this method, it's not generalizable. Okay? So, because inverse of some of the methods is not computable. Okay? So, in practice, the method, the other method, let's say, which is used, is gradient descent. Like, each of those has their own virtual, right? So, the merit advantage and disadvantage. Okay? So, Now, let's discuss about the gradient descent and then we can compare in which occasion we can use this for any mutual case in grad. Okay? So, gradient descent is very simple. Let me explain that. What is this? Like, suppose that you are in, you are in, I'm going to ask someone. Something is in for this one, right? Right? So, noise is good. Then, I'm fast. Okay. So, what I mean is, is that you have a function, right? This function is fx. Okay? Suppose fx is something like tb of your value. It will be different. But for now, suppose it's x. Then you are, you want to see that what is the, and if it's x, and if it's the y, you want to find the optimal form. Okay? In other words, the minimum form. Okay? Where is the, the minimum point of this function? Okay? So, we know that if you get the derivative of this one, and you put zero, you can return something like this. Two x is zero, and x means zero. Okay? This is for simple functions. So, what is the method in general which we call that one as gradient descent? So, suppose that you are on top of a mountain. Okay? Then you want to know that, what is the, like, a flat area, right? You want to go down. And that's foggy. You don't see anything. Okay? Imagine like that. But you want to get it that, I mean, in like a quick stop. Okay? So, you don't, you don't want to get stuck in somewhere which is not the ground. Okay? What you do, like, what you can do, the easy way is that, you find the steepest, right, steepest way. So, you are here, and then, maybe there are different ways, but you check which way is the steepest. Okay? You are here, maybe there is one way on the mountain, there's another way, there's another way, but you know that if you continue that steepest way, you're going to move the point which can be closer to the ground. Okay? So, you are here, you take the steepest way, the one which has the most, the steepest one, and then you put here after two, three steps. Okay? Or after one step. Now you are here, again, maybe here you have different ways. Okay? So, which way are you going to take? Maybe you have one way here, it's, slope is like this, the other one is like this, and the other one is like this. So, again, you're going to take the steepest way. The one which has the highest slope, and then you get here. If you keep doing after a while, what's going to happen? You're going to get the, yeah, the bottom, okay, or ground. That's the same idea here. So, if you want to, what does the gradient descent tell us is that, suppose that you are in a random point, okay? And then you want to get here. What you can do, you can start from here. And then you have, okay, so here is your, okay, let's call it as, okay, or your current point, X current, okay? X. So, you are here, and each time, what you need to do, you need to move toward this, right? This is slope here. Is it positive or negative? Yeah. Downward. Here is the upward, okay? So, I have X. I know that slope here is negative, like, let's say, the minus, okay? The slope here is minus 2. This X minus value to this, do I go this way or this way? It's going to be this way, right? You just suppose that this is 10, and then slope here is minus 2. And then I add minus 2 to this one. What's going to happen? Rather than going to this way, I'm going to go this way. And it's wrong. So, what is true is that I need to add 1 minus 2 here. And what it tells, it's going to be 10 plus, 10 plus 2. So, the next point here is, again, what is the slope here? Is it positive or negative? Negative. So, what I need to do, I need to take this 12 and add the slope there, okay? Which is, again, minus 2. Because, again, I need to add this. So, I want to go this way, right? Should be minus 40. And I continue, okay? So, I continue, and then what's going to happen? Always I go this way, this way, this way, here. Right now I'm here. And I keep doing, if I keep doing this, what's going to happen? I'm going to get the optimal, okay? I'm going to get the optimal, okay? So, or maybe it's, x is 0, maybe it's not 0, just like something like 20, okay? Just like that. So, what I did, I did something like this. Each time, if you consider this t, this t, each time I added something like the slope of this, f prime of x, okay? And it gives me. And it gives me x, t plus, okay, my position in the next step, okay? So, it's just this, this one, okay? Sometimes, in reality, it's not like exactly like this. We have a coefficient because always it's not like other steps is big steps, right? Other steps might be like a small steps. Because suppose that you are on a mountain, right? So. So, you don't, it's not, it's not like a safe way to take a big steps, right? What you want to take, take small steps, small steps, and because, and then you're going to get the ground, but it's a safe way. Like, if it's the same in creation, you can think about multiplying this one by something like one coefficient, okay? So, like. And we call this one as learning rate. Learning rate. So, the value for lambda is small values, okay? Like, say, tell, consider lambda here is 0 point, what's going to happen? If I am at 10, my next point is going to be like 2 times 0.1. So, it's going. It's going to be 10 plus 0.2, rather than 10 plus 2. Okay, so I'm going toward the 20, but with small steps, okay? So, this is, this is called the learning rate, okay? So, in other words, I'm here. If this learning rate is small, rather than coming here, I'm going to be somewhere in here, okay? Mm-hmm. Mm-hmm. Mm-hmm. Mm-hmm. Mm-hmm. Mm-hmm. Mm-hmm. Mm-hmm. Mm-hmm. Mm-hmm. Mm-hmm. This is going to be small steps. What is the, what is the, I mean, disadvantage of this? Let's first talk about the disadvantage. Think in reality, what's happening if your step is not safe? It takes time. It takes too much time, you can imagine, to get here. Ah. Right? Yeah. This is the disadvantage. The advantage here is that you're going to get your objective point in a safe way. Okay? So, even if it takes time, but it's better, at the end, I got to get my, my objective, right? So, what if this lambda is bigger value? Yes. What's going to happen? Yeah? What would be the issue here if this lambda is getting bigger value? Okay? So, think about this. You are here. It's 10. If lambda is bigger, like suppose that lambda is 10, okay? So, you are here. It's 10. Okay? So, you are here. You are here. You are here. You are here. You are here. You are here. You are here. Okay? Or 5. It's going to be 10 plus 5 times this degree, which is 2. 2. It's going to be 20. 20. So, what's going to, let's say that, rather than 5, take it 6. It's going to be 22. Right? So, that means that I'm going to jump from here to here. Yeah. Yeah? So, what happened? You got 5 points. I missed this one. Okay. Again, here. What's going to happen? You missed the point. It's going to be something like this. Like a zigzag. Right? Or maybe I can't do it. But that's why, like, this learning rate is important. Yeah. It shouldn't be very big value. Big value is going to be something like this. You have to balance. Sorry, what's that? I mean, you have to balance the small and the big. Yeah. Exactly. Like a small learning rate shouldn't be too much small. On the other hand, it shouldn't be very big. You're going to have a very big learning rate. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Just join. Okay? Yeah. Yeah. Yeah. Yeah. Yeah. Right. Yeah. Yeah. Yeah. Yeah. Yeah. Are there less things to say? Yeah. Yeah. Exactly. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. What is the last acted? Oh. Oh isn't it? Yeah. going to reach here. Okay, what is the slope here? Is it negative or positive? It's a positive, right? It's a kind of like upward, right? Then, if I add to this point, the slope of this point, which is again 2 times 2x, right? So, it's going to move me this way. But I have one negative, and that negative helps me to move this way. Definitely. I just want to mention that this equation, this equation is a kind of like general equation. It works for every point, sorry. It should be minus, right? Yeah? This minus helps me. If I have it at downward, minus minus is going to be plus, and I'm going toward, like, to the right side. If I'm here, because here is just a slope is positive, minus times like a minus times positive value is going to be minus, and it moves me, push me to the left, right? This is the technical called the gradient descent. It's a repetitive algorithm. You start from some point, let's say one of these points, and then keep doing this, and based on that point, you update your position. After some, iterations, you can get the, get the mean. Okay? What would be the stopping condition for this algorithm? Finding the minimum one? The change is not, I mean, the change is not significant, right? The change is not x and x t plus 1 and x. It's almost the same. This is one way. Or maybe after 100 iterations, we say, okay, after any iteration, we say, okay, stop. Or whenever you see that, it's returned like a minimum value, like we are looking for 0, right? Whenever f getting 0, because we have x, we can apply that one on f, and see that where it's getting 0. If that point leads to 0, we can stop. Okay? So there are different ways. So suppose that after some iteration, we continue, continue after 100, for example. Now we have some idea about the gradient descent, and this is a linear regression, right? So we have some point, and I want to pick a function, pick a linear function here. Okay? So we know that after picking this function, for some points, it's going to be error is going to be probably 0, but for this point, this is just a person. So for this point, error is 0, for this point, we have error, we have big error here, not too much big error here, so we have some errors. And the error is the difference between the predicted value and actual value. Okay? So actual means our ground truth, and predicted value is the output by the model. Yeah? Okay. So we know that we have some error, so if I put all this error together to all the points, it's going to be this one, okay? And this is nothing more than the sum of the squared error for this point. Right? So this one minus this, which is squared off that. And then we go to the next one. And that's one of them. So, for example, I have 2, for 2 I've got 2.2. So error here is 0.2. I get square of that square is going to be 0.04. Plus, same thing for the next point. Plus, for the next point. And I get it, and then it returns 1 over 2. Right? So, if I divide this number by n, it shows that the average error. So, let's consider this one as some of the squared error. We know that we have this, and we want to minimize it. Right? This is our objective. Okay. Again, I have a function here. Right? And then just replace y hat model of 2 by this function. Right? So, and that function is our linear function. A times x plus 2. And in other words, I want to minimize this, or I want to minimize this. We don't have 0. Just consider this. Okay? So, y i minus a x i plus p squared. Okay? One more point. How's this function? Is it linear? Is it polynomial? It's squared, right? It's squared. Okay? So, because that's squared, then its shape is going to be something like this. Okay? It's not like a plane. It's going to be something like this. Okay? And now I want to get the optimal point for me is the point which makes this error function 0. Okay? So, I mean that suppose that this e is our function x. So, this one is also squared, and this one is also squared. You can consider this function, and something like this. And what we are looking for, we want to see that where this error is getting 0. Or, in this context, where our function f is getting 0. So, I can apply the same idea of gradient descent on error function. So, what I need is that, like, again here, we want to know that in each point, the error of the model is getting minimum. Okay? And based on that one, we want to find a and b. Okay? a and b. Those are the coefficients which makes our function. Okay? So, what I can do here, I can apply the same idea. I start from one initial arbitrary point. For what? For a and b. So, suppose that I start from here, and then here, I calculate the value of e. Okay? So, and then, I change a and b. How I can change? I find a slope with respect to function e, and it reaches values. It's the slope of the function. Right? And then, if I apply that one on something like this, okay, what's going to happen? I have currently, suppose that I don't have a. I have just b. Okay? So, for b, I have just a. Okay? I don't have the b. Okay? It's kind of like a. Okay? So, what's going to happen? I have one value for a, and then, after taking the derivation of that function, it returns something like f prime to b. And based on that one, I can know that what will be the next one. If I'm here, I'm going to down. Down. And then, it leads me. The error is zero. Okay? And, okay. When I got that point, that means that, okay, what happened? I have, I, I, I checked different a and b's, and at the end, I got the best a and b. Okay? What was the best a and b? The best a and b was the point that, for each function, a and b, or error, getting zero, or minimum. Minimum, okay? Because if you continue to watch, it's getting zero. Otherwise, suppose this example, I think this one can show what, what I mean. Suppose this is the, function f or function e. Okay? And then, you want to know that what is the best x and y, rather than a and b. Okay? So, what I can do, I can find the, derivative of this function. Because I have two variables, I need to get the derivative with respect to x, and with respect to y. With respect to x, it's called norm of f, or derivative of f with respect to x, derivative of f with respect to y. So, what is the derivative of this function with respect to x? x 2 times 0.5, right? 2 times 0.5. y times x. Okay? How about with respect to y? 2y. Right? 2y. So, because this one is getting constant when I'm talking about y. And when I'm talking about x, this one is getting constant, so I don't consider this y in this, and I don't consider x in this. That means that just for this one, I need to find the derivative of this. Okay. So that's why, like, suppose that I start from 10, 10, a point right here, x is 10, and y is 10. A point right here. Okay? And then I want to see if I can get to the minimum. So, I have the gradient, we call that derivative, we call that one a gradient. So, gradient means that slope of the function. Okay? So, our gradient in this point, uh, sorry, our gradient is 10 and 10, it's gonna be 10 and 2 times y, which is 20. So then what I need, I just need to here, okay, take this, plus the gradient. Meanwhile, we need to multiply that one to our learning rate. So I'm in something like this. So every point, next point, or next point, next position, is gonna be current point times learning rate, okay, times the gradient. Okay. So, so gradient isn't is nothing more than this. So, on the, example on the, uh, point 4, rather than t, probably I have used once, I have used, uh, x, right? So, x, t plus 1, or n, n plus 1. Same, same situation. So, this one is very simple algorithm. You start from initial guess, initial random point, and then you calculate gradient of, uh, each point, like the current point, and then after that point, you update this. Okay? And then you repeat step 2 and 3 until one of these criteria is next. After maximum number of iterations, and then you get the maximum number of iterations, for example, after 100 iterations, or your step size is smaller than even full x, for example, as your, uh, classmate mentioned that, like, when the change is not significant. Okay? You see that x is 1, and, uh, x, xn is, for example, 1, and n, xn plus 1 is 1.00001. So, they are very close to each other. Not too much difference, so then you can stop. Here, there is a nice example. I say you can go with this example and see how the gradient descent works. So, suppose that this function f is x squared minus 4x plus 1. And you want to see that, uh, and this is your error function. And you want to see that what is the optimal x. Okay? So, your guess point, or start point, is 9. So you start it from here, and then you want to get this point. What's going to happen, I get the derivative of this function, it's going to be with respect to x, because I have only one variable, it's not like with respect to x and y, or x1 and x2. I have just one variable, x, right? Okay? So, the derivative of this function is going to be 2x minus 4. Then, my start point is 9. I get that point as, I consider that point as x0. Okay? So, in x0, this value is getting to be right. Yeah. In x0, which is 9, this value is getting to be 2 times 9 minus 4. As you see here, 2 times 9 minus 4. And, uh, I multiply that one by our rate, which is 0.1. That's the assumption. Okay? Then, if I see the next point, it's going to be 7.6. That means I'm here, the slope here is positive, so minus positive, which leads me going from 9 to 7.6. And again, the 7.6, which I'm here, I can continue. 7.6 is applied on this, okay? So I have the current point is 7.6, this is my burning rate, and the uh, gradient is 0.7.6, because currently I'm at 7.6, so it's going to be 2 times 7.6 minus 4, which pushes me to 0.640. And if I keep doing this, you see that each time, it helps me like, it pushes me from 10 to 6.48 and 5 points I'm saying, and at the end, it leads me to uh, the minimum point of this, right? Which is, what is the minimum point of this function, right? It's equal to. So it pushes me to 2 at the end. Okay? How many steps? Like, let's just solve this three steps, okay? Like, after 24 steps, the result is 2.04. 2.04. Okay? Not exactly. If I keep doing, maybe I can get the exact point. Make sense? Any question on this? You have question? Yeah, which one? Not clear. 0.1 is our assumption. I say that's like a learning rate, okay? 0.1. This is something that's, you know, like a, it's a kind of like input, or assumption. It's okay. Take it 0.1. Or maybe take it 0.2. Okay? It's some assumption. But the other, others are not that. And the another assumption is that I start to solve x equal to 0. You can start from any point, and then you're gonna get the local, uh, you're gonna get the optimal for your x equal to 0. The takeaway here is that as you see in this example, if I have any function like this, not any, any function, but the function which is, uh, you can calculate its derivation, okay? So in other words, this is the depreciable. The function should depreciate, okay? Uh, means that I can find its, uh, gradient. Like, this is straight. Okay? So, if I have any function in the format of, format of, for example, a square, or any differentiable algorithm, I can find its optimal point, or the point in which that function is getting, like, a minimal using gradients. One of those examples, like, this is one example, but we can extend, I mean, we can generalize that one into our arrow function, right? So, because arrow function also, you see here, it's kind of like a function like this, and it's differentiable. Okay. So, for gradient descent, uh, we have some limitation or requirement, let's say. Uh, the first requirement is this. Uh, the function should be differentiable, right? For example, this is differentiable. This one is differentiable in any point. In any point, you can find this slope. In any point, you can find this slope. But for this one, you can't find this slope in x equals 0. Okay? Some of the functions, like this, or this, or this, they are not differentiable. Okay? If it's not differentiable, you can't find this slope. Right? Gradient. If you can't find the gradient then you can't calculate or apply this function, this equation. Make sense? Yes or no? Okay. So, other thing is that it should be convex. Okay. What's the convex mean? Okay. For example, this one. This one is differentiable, right? Because you can find the derivative of this. Derivative is 2x minus 1. In any point, you can find this one. So, or maybe you can take the derivative of derivative which is like this. Again, if you take the derivative of this, you can take this, it's getting 2. Okay? So, the function which has the derivative everywhere, the second derivation is always greater than 2. Okay? So, maybe you can ask how I can understand that function is differentiable. So, you can take the derivation 2 times and it returns the something like this. It's greater than 0, positive value. So, then you finish it. Okay? What is the convex? The other requirement here is that it should be convex. Okay. So, the convex is something like this. If you take any two points inside of this function, right? One point here, one point here. And then connect these two points together. Okay? Like, look at here. I have this one point here, another point here. Then I connect these two points together. That line is inside of my function. Okay? It's going to be something like this. That's inside of that pole, right? Okay? That's a ball. Okay? So, that is its convex. But how about this one? Is it convex? No. The reason is that we take these two points and if you connect them together, this line is not necessarily inside of it. Okay? Inside. Bigger. Okay? Inside. So, that's why, like, we know that this one is not convex. Okay? So, these are some requirements that we need to have. That means that we can't apply gradients on any function. So, this is in the format of, for example, like, squared or something. So, for example, right, how nice? No. No. Like this. That's one of the requirements we need to Okay? Just simply What is important, so, again, let's focus on this equation and the importance of learning rate. I showed on the whiteboard like what's going to happen. Suppose the learning rate is 0.1, this small value. Then you see that if you go slowly, you start from here and you slowly go toward the optimal point. If this learning rate is bigger, your steps, your jumps is going to be bigger, right? So, that means that you jump from rather than going from here to here, you're going to jump here, okay? And then, probably, you can get that optimal. But if it's a very big number, it's going to be like rather than 0.1, it's 0.8 or 10 or 9 or 0.9. Okay, what's going to happen? You're here and you jump. Okay? It's going to be something like for example, it shows that in this example, if it's the learning rate is 0.8 for this function, after 15 steps, you're going to get the optimal point. Right here, after 8 steps, you can get it. Or, if your learning rate is very big, then the number of iterations takes more time. What is important here is that the learning rate what is the balanced learning rate? What is the best learning rate? That's why we don't take it very big. We don't take it very small. Okay? So, major is 0.1 and it's good. You take that one as 0.01, it's going to be a little bit small. Okay? So, again, here it shows that the learning rate is too small, too slow to convert. Maybe reach the maximum iteration before convergence. That means that you said that I'm going to stop after 100 iterations. Okay? So, if your learning rate is small, and you stop after 100 iterations, what's going to happen? Maybe you can't get the optimal point. Okay? And if it's very big, may not converge to the optimal point, because I'll jump right, right? All these moves. Question? Okay. The second point is the point which is you see that, like, the whole silo, right? It's a point that's something like this. Just want to mention that this point is not the local minimum. So, because the minimum value for this is here, we have a point here, its gradient is zero. Okay? That means that it's going to be local optimal, but it's not our global optimal. So, if we have some saddle point, maybe the algorithm can make a little bit mistake. So, that depends on the data that we have. Okay. I think this one is just a summarize everything that we have here. So, the idea here is that, suppose we have some point, a couple of points, and then I want to see that what is the best bit flight. Okay? So, what I do, I start from some, suppose, m and b. Okay? Like our a and b. a and b. This minus 8 and minus 8 is going to be something like this function. And we want to see that what is the best. Okay? So, I want to change a and b. How I can change a and b? If I need to find the gradient of error, and see that like I can step by step update a and b. And continue this until I get the optimal a and b. In other words, if I have something like this, after one iteration, maybe change. Okay? So, I have a function like this. After one iteration, maybe, rather than this, rather than this, it's going to be changed a little bit like this. Okay? After the second iteration is getting something like this. And if I keep doing this, it's going to be correct or close to correct, let's say, line. Okay? So, for this line, we have error of this value. This much error. Okay? If I change a and b, my expectation is that the error should be previous because of the nature of gradient descent also. Okay? So, what's the connection between here? We have some points. We want to see that what is the best line. Okay? So, for each line, we have one error. I can consider that error and the function that I want to optimize. Okay? So, because it's s squared, it's going to have a shape like this, then I want to see that where this function is getting minimal error. Okay? If I find that point, in other words, I'm treating this line. I'm changing a and b. Okay? I start from this point, I'm keep doing, and at the end, it leaves me a point which is error. Okay? So, it's important to see how we can connect these two figures together or three figures. We want to minimize, we want to find the best line. The best line means that I want to minimize this error. To minimize this error, I use the gradient descent. Okay? Make sense? Okay. And because only this function is convex, right? So I can I can apply the gradient descent. Okay. So, now that's the point that I can think about, okay, which one is good? Gradient descent or least square error? Okay, close one. So, for gradient descent, what is important? I need to find the I need to choose a learning rate. It shouldn't be too small, it shouldn't be very big. I need to Okay. And the other thing is that it needs many iterations. Okay? It needs many, maybe takes time. Okay? So, on the other hand, for least square error or close one, you don't need to choose any learning rate because we don't have any learning rate here, right? So, what is what we have is just this. Okay? Take the point, construct x, construct y, and then multiply them together. The only cost here is the cost of finding inverse. Okay? So, one limitation here is that okay, the advantage is that it doesn't need any iteration. Okay? x transpose x is okay. We can, with any square we can find because multiplication of two functions, two matrices. Okay? Every row, every column. Limitation here is that some of the matrices is not invariant. Okay? That means that you can't use this anytime. But this one is okay. You can use for almost all the data for data sets that you have. Okay. On the other hand, another limitation here is that if n is big, again, calculation of the inverse is it's kind of complex. Okay? So, that's why, like, when your n number of points is higher, in reality, we have too much points, right? So, we don't recommend this one because it's getting very slow. Why? Because the calculation of the inverse. And also, the other limitation is that always inverse is not available. Okay? So, that's why we use the gradient. We send majorly in practice. Okay? And the recommendation here is that if your data points are under 1000 points, you can apply NSS. Otherwise, go to gradient. Okay? Because when it's under 1000, it doesn't take too much time to calculate those matrices. Express with x in there. Other than that, it's gonna be computationally expensive. So, that's why, like, we prefer gradient. So, which one is used in practice? Because in practice, the number of points is not 1000. It's not this simple toy problem. This is a simple toy problem. Yes, if they take a point, it's probably just 1000. And it's okay. You can use the NSS. Okay? This is correct. Okay. I think last time I talked about the over-fitting. Right? But let's focus a little bit more on what is over-fitting. Suppose that you have some data points here. Like x and y, right? And you want to fit a function. Because it has less than 1000, I can apply simply the formula x transpose x inverse x transpose y. Okay? So, that means that suppose that I want to find the best polynomial of degree 2. Okay? Because I see that this is not exactly linear. What I see is that something like polynomial. Right? It's not linear. So, let's take this one. Just that degree of 2. And based on that, I can just fit the values. Right? So, for each x, I have its y. So, I can calculate x squared, x and 1, and this is the y. Maybe you see that sometimes this 1 are here. Okay? It's also okay. Then it's going to be 1, 1, 1. Then x, then x squared. You can exchange the values. Okay? First column is 1. That column is x squared. Like what we had over here. Right? Okay? So, theta 2 is x2, x1, and this is just the rows constant. Okay. If I go based on that, if I go based on this, x, and then y, and then if I apply the old values, the optimal coefficient for me is going to be 0.68, 1.74, 1.73, and then, in other words, I will have order-2 polynomial, polynomial of the , right? Any questions so far? Is this clear or not? Is there any question here? Just interrupt me if there isn't anything clear, just interrupt me because I don't want to continue if someone has some questions. Just ask your question. Any type of question is okay, okay? You don't have simple or complicated questions. What I did is want to summarize. I have some points, and then I assume that the function here is the, for example, degree of 2. And then I find that order 2 or degree of 2 polynomial. Okay, how? Using the closed form. Or maybe you can apply the gradient descent. Okay? Both of them are going to return a function like this. Okay. So what I have here is that, like, some of those are not exactly feet. All right? So some of those are very close, but this point is not. Okay, let's increase the order from 2 to 3. Okay? Is this one better? Is this one better or this one? Visually, I see that. Like, still, I have some error. Okay? Maybe this one is better. Okay? So degree 2. Not too much difference, right? So that's why, like, they are close to each other, the amount of error they have. Let's increase that one to this one, order 4. Okay? If I have more points, probably it shows that the degree of 4 is better. Okay. But. What I'm doing, I'm increasing that one to order 5. Order 6. You see that I want to cover every point. Order 7. Okay. Order 8. Okay, what happened in order 8? The function I feed is almost covering every point. Exactly. For this one, this one, this one, the error is zero. Maybe I have just some small, very small error here. Okay? Then, if I increase the order from 8 to 9, it can cover, it can feed every single point. Is it good? It's not good, right? Why it's not good? Because if I look at, I look at these data points, I see that, like, generally the type of relationship should be something like this. Maybe this point is an outlier. It's a kind of noise. Mm-hmm. Or maybe I have. Let's say I have one point here, one point here, or one point here. If I increase the order, what's going to happen? It's going to learn every single point. Every single point. For example, if I have one point here, it's still this function. It's going to learn that one. But that's a kind of, like, noise. Maybe I have some points. Some of the data points are kind of like outliers, right? For example, you are in a class. Okay? So everyone is good, or average about, or around, so maybe someone is, like, kind of, like, very good, or someone is not very good, right? It's opposite of each other. But if you want to see the function, in general, you should know that what is the best function for that. Some of points can be outliers, okay? I'm talking majorly about the, for example, the point, which is far from the average, or far from the, you know, how you have, okay, far from this, like, very bad, for example. So those are some, let's say, outlier points in general. Or maybe, like, in any context, you can make maybe some show. Yes, if you increase the degree of your polynomial, maybe you can hit every single point, but some of those points. Okay? Noise. Outlier. Noise. Okay? So we don't want to learn the noise. Okay? So what would be the issue if I learned the noise here? For example, here. In this example, why do you think that it's a problem? Because suppose that next time I want to add a point here, like this one, here. Okay? Do you see the cursor? Okay. Okay. Suppose that I have a point here. Okay. Okay. Okay. This is a common point. This is a common point, right? But that's an unseen point. I have a new, let's say, the sample. For that sample, I want to see what is the y of that point. Okay? So if I have a point here, my expectation is that that point's y should be, because you see that's a kind of, like, increase, right? Something like this. If I have a point here, it's y should be something this error. But based on this, based on this, it's y should be a point right here. You know what I mean? Because you see that this function is, like, a kind of, like, a sine is all right. So it's going up, down, going up, maybe some down here, because we don't see. So if I have an x here, it's y is going to be negative. Okay? Okay. But for that x, it should be something around here. Okay? But what if, rather than this, I had a simple linear function? Which one was better? Linear, right? A linear function. Or, I mean, if you have such a thing, if you have such a thing, or something like this, degree of three, let's say, just degree of two. Okay? Okay? Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. It's not that. It's not that. Okay. Okay. Okay. If you are going to consider any point here, its output is going to be here. Okay? You consider any point. Like, if I have a point here, its y is going to be here. It's something we expect. Okay? What is the, what is the learning for? Take away. The takeaway here is that if you make your model complex, if you make your model complex, in other words, it is there. order of your function, right? It's good, but it's just for your training data. You can fit, you see that you can fit your function on any point. For that one, you have the target, okay? But what if for unseen data, okay, or for your test data? If you have a new point, which is not available in your initial set, if your model is complicated, those unseen data, there's a high chance of error. There's a high chance that the model cannot detect those very well. Because you made your model complex and you learned the noises, so those noises is going to be something. Okay, this issue is called overfitting. So overfitting means that your model performs well on your training data but not necessarily on the test data. In other words, the amount of error you see on training data is lower for test data is higher. It's not good. It's overfitting. This model is nothing. We don't consider that. A good model is that. Like what if, for example, let's say I have a model. This is a question for you guys. So I have a model. So why don't you change your location? I'll ask you to stay here. Okay, no, no. Just, just. Okay, so suppose that you have a model A. Model A and your training data, you have 2,000 training data, okay, like 2,000 points like this, okay? And then you fit a model a polynomial of degree 9, or whatever, and the error you get is, for example, 10. Okay? The error that you get as y minus y ha Click and then from you normalize that point to the number of point and you get 10. It shows that, for each point, the error in average is 10, okay? This is the model A. Then you apply that one, and then overfitting. that model in your on-scene data. And you got the error of like 200, okay? So 10 for training data, 204 testing. The second model is like this. It's a simple model, just a degree of 2. And you got the error of 30 for training and 4 tests. 30-30. Which one do you prefer? Second or the second model? Why do you think, why do you like the second? And then what is important for us? Yeah, stable, but what is important for us? Suppose that, let's make example in reality. Suppose that you want to create a machine learning model to predict the price of houses in Great Bank, in downtown Vancouver, okay? You have some points. And then you train a model. It works perfect. But you know, those data, you don't need those information. What is important is that it predicts something, predicts for something that you don't know the price of that. You know what I mean? So I mean that if your model that was perfect on training data, because I don't need, I know what is the price of, for example, this building. I don't know what is the price of the second, okay? So that's why, like, training data is not something that we do. It's an indicator, but it's not everything, okay? So our prediction model, like, suppose that you have prediction for weather, right? You want to forecast what will be the next week's, I mean, temperature. And you have a machine learning model. It's trained on something. For those trained data, of course, it should, works perfect, right? 100. Something like memorizing. Okay? The good thing is that it should reason. It should, something like, can predict on seeing data, okay? That's why. In reality, what is important is that the model should perform well for both training and test. If it's like a training is very good, but test is not good, it's, I personally prefer a model which train and test are, Great. like, we, are, okay. Then training is perfect. So is better than, much better than, for example, 10 and 200. for train and test. Is much better than error of 10 for training. But for, we, call this, what as an over-fitting, in other words, we's, we can, of our model is not generalizable. Okay? So generalizable means that our models are, you're wrong, okay? ORT model should perform well on test data as well. It's something that's like a very common. And also like if you go and work in industrial, in academia, it's something that if you make a machine learning model, first thing you need to check, okay, how's my model? It's good. Is it good for the test data as well? If not, change it, think about your model, change it to something, make it simple. Maybe you have, you made it very complicated. In reality, something like, sometimes, you know, you are overthinking on something, right? You don't need to think that much. So when you are overthinking on simple problems, also, you can't make a good decision. Sometimes you prefer, okay, let's take it casual, right? The simple thinking is the best, right? But casual is not the best. It's not also okay, like some say teaching, right? Machine learning models, we have the same, okay? Overfitting is a very common question. In interviews, like, as far as overfitting, the first question, you go to machine learning or data science, one of the theoretical questions is that, suppose you have a model. How do you know that there is overfitting? The answer is that, okay, check the error between train and test. Okay. How do you know that they are close to each other? No, it's overfitting. How we can prevent overfitting? One way is making the model not too much complicated. So there are some ways that we can do that. Okay, so I think a lot of them are talking about the same. You want to find the hypothesis, hypothesis is the model. Which explain perfectly on training data, but cannot generalize to, it's a problem of overfitting. So, yeah, in this example that we had, we have, for example, lots of parameters, those dates, W0, W1, right, those coefficient. So how does this match the data point exactly? Yeah. For order of nine, but it's one everywhere else. So 500 everywhere else, right? Mm-hmm. It's one of them, right? It's one of the important points in machine learning. Okay, here is another example of overfitting. So degree of zero, what is the degree of zero? Flat, right? Mm-hmm. Just a flat line. Flat line. Right, this line? Yeah. It doesn't have any slope. Yeah. So when the degree is zero, you start from degree of zero, not degree of zero or zero. Then we increase to degree of one, and find the best degree of one. How? We can use the gradient descent, or we can use the closed form, right? So return this line. I see still, I see that it's not with feet. Increase that one to two, then three, right? The red line is our feet function. The green line is, in reality, where one is at, okay? Ground control. Okay. So still we have some error, but if I increase too much, it's going to be degree of nine. We're going to be learning every single points, and it's overfitting, okay? So the higher the degree of polynomial M, the more degree of freedom, and more capacity to overfitting. There is one slide here, which I believe, explain that one. Yeah, I may need some time for this. So, but in general, I want to say that, you know, I want to explain this, maybe up here, okay? So one way to prevent overfitting is making the model simple, okay? Making the model simple means that I don't want something like this, very simple, very simple, but something that is not complicated, but at the same time, I don't want to see too much error, okay? So. If you're not having too much error, what I can use, I can find the error of the model, okay? Like y minus y hat, right? So I want to minimize it. If I minimize that one, in other words, the error of the model is going to be small. But the error of this one, and the error of this one is also small, but this one is almost zero. Here, probably, you have some error like this, okay? At the same time, what I can control is that how much, what is the summation of the coefficients? Is this going too much up or down? That means that, like, here, if I go on this example, okay? So if these values are going to very big numbers, very big numbers, it shows that you have w2, w1x, w2x3 plus w4, w2x squared plus w3x cubed, right? If these values of w are getting bigger number, it shows that your model is complicated, okay? In other words, going up and down, too much up and down. So it's too much up and down. It's too much up and down means that, this values coefficients are big number. If you make those values smaller, your model, your model will not have some jobs like this, okay? So what is important here, I want to mention this. At the same time, just think about this. At the same time that you want to decrease loss function, loss function is error, you want to have those coefficients small, as small as possible, okay? As small as possible. So if I don't have this one, just have the loss function, what is this object function? Objective function that we discussed, error, right? Y minus y hat squared. But it's a multi-object, so I say that at the same time that I want to keep the error, like, small, I don't want to have, you know, very big code, okay? Rather than minimizing this, I can minimize this too, okay? That means that at the same time, error of the model should be small, but I don't want to see these big jobs. To control those big jobs, you need minimize, okay? Take code like that, if you have, for example, w0 plus w1, because it could be positive or negative. So we take the absolute value, okay? Forget about this lambda, what is that for now? Suppose lambda is 1, okay? So this is called the regularization. Regularization means that you want to regularize your coefficient. Rather than absolute value of coefficient, sometimes we use the square. So this one is called l1, this is called l2 regularizing. They're not, in reality, too much, they're not the same thing, but rather than summation of coefficient, we have the summation of this square in l2 normalization, l2 regularization. If you use this one, either l1 or l2, it tells you to prevent the overfitting. So how do we combat overfitting? Regularization is the solution, okay? So what is the lambda? I mentioned the lambda, like, consider that one as 1. So, if lambda is something small value, what's going to happen? It's like lambda is getting to, for example, 0.01. Why? Because we give the weight, we give the more weight to plus function, right? Suppose that this is 1 times plus function plus 0.01 times coefficient. It's almost nothing, almost. So, but if it's like a 1, this one is 0.01. So, 5, that means that we give the weight to plus and add the same time to this one, 2 times this. It's not like 1 and 100. If it's like 1 and 100, what's going to happen in 1 and 100? Like, suppose the opposite, right? So, I give them more weight to coefficients. What's going to happen? Note that the first thing is to control the error. The second one is to control the overfitting. It is to control the error. Right? Right. If you make this one bigger, you will not have over-fitting. It means your model is coefficient is going to be very small, slow, right? It's a kind of like plateau. It means that, like, you give more away to this one. It means that, like, the responsibility of the second sentence is to prevent your model to be complicated. If you give more away to that one, your model is going to be too very simple. Very simple, okay? So, on the other hand, if it's a small value, that means that you don't consider that sentence, and your optimization is just based on decreasing the amount of error, or whatever model, right? So, no matter that that's complicated or what. So, in other words, it's very small, it seems that we don't have this. So, that's it. So, that's important to know that. Also, lambda here is another, let's say, hyperparameter, like this learning rate, okay? And we need to control. We need to, we call those as hyperparameters, okay? Hyperparameter can be this lambda, or can be this learning rate, okay? Any question on this? So, the first sentence, just minimize. The second one, prevent you to have the over-frequency. Because the second one helps you to not to have big jumps. Coefficients do not be very big, means that you don't learn the noises, okay? The square is called L2. The first one is called L1. Sometimes we call that one. Yeah, we call L1, and the rest are L2, okay? So, we call. We call this one as, and this one is called range. That's the problem with the first one. Okay? So, for example, if you look at this, this is for the L2, and this is my circle, and this is your last function. You want to find the balanced one, which is here, okay? But here, this is your, you see this diamond. It is absolute, and this one is the optimal one, the optimal coefficient, okay? So, W1 and W2, but here, the optimal coefficient, anyway, just simply on this, and what is this, and it's important, okay? And we discuss about this, if we don't include this. Question? Yes. Yes. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Okay. Well, first of all, if your data is kind of like just one, like this one, like x is just one dimension. Yes, maybe you can, you can see that, those points, and you take those and remove them. Okay. One way, yes, of course you can do it. But sometimes it's not like this. You have x, you have 100 columns, and based on that one, using. you want to just predict normally sometimes it's not easy to detect those numbers but for example clustering is a way to detect those numbers yes you can find those isolated points remote dots and then it makes it easier to predict them or train them visually you can just check one or maybe two right or maybe just the three okay but it's no more that's what but in reality most of the time our data like for example in the in the banking system we want to check this customer at least or not not just based on one credit score you know lots of lots of columns again you want to calculate premium in insurance not just depends on one column there are lots of other features neighborhood uh education uh lifestyle everything again and then the size of for example authority this is all i think if you want there's too much cover really just going to one column not really but yeah maybe it's a good idea or escape the data apply some algorithms to detect the last layer remodels and yeah i mean good idea but not really any other question good question any other question okay you mean that what if the degree of polynomial is higher okay if your polynomial is higher what's gonna happen okay you're gonna have over two but your point is that what if the what if the lambda is also getting higher in that case at the end it doesn't consider this one because it gives more weight to this one to minimizing this right simply means that if this one is 100 and this one is one it doesn't care about the error of the model you care about the makes the model simpler like a flat line or something because this one should be something that okay and that is still if even if it's a daily of nine but it's gonna be something like some line simple line yeah good question anything else okay so for now very good you can stop here take a break um and get back after 25 minutes okay 8.4. 8.4. 8.4. 8.4. yeah you may sit square is music so um you should be a plus oh yeah that should be a comentarios