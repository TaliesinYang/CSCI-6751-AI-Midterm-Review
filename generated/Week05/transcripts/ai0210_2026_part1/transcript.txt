the training model and i cannot like use that one for the prediction on the testing got it so validation is different from test data it's a part of our train data but it's used to evaluate the training okay the train model itself makes sense okay so that's why you see that sometimes uh you see validation and tests maybe sometimes you just see three and this okay uh it depends okay so now let's focus on the uh regression model there are a couple of uh metrics here which is uh used to evaluate the performance of the regression model so the first thing is mae mean absolute error means that y minus y hat divided by okay find the absolute error the difference between the target and prediction of the model the absolute value and then sum up this for all the observations for all the records and at the end okay it's called the mean absolute error it is very common okay so the other thing is is called mean squared error rather than y minus y hat you find a y minus y hat squared okay for each data item so for example data item one is going to be y 1 minus y hat 1 squared plus y 2 minus y hat 2 squared plus a right and then divided by the number of observations okay so it's called root e squared and then and then you can take the square root of that okay because that's in the degree of two then you can take again uh you can take the square root okay and we have also r squared we have adjusted r squared so you want to focus on uh each of this and explain so the most common one is the mae and rmse this is the uh equation that we have right so as i i made this is the simplest uh version of the you know those uh metrics so y minus y hat for all the data items and then divided by the number of so y here is the front row y hat is the model prediction okay so the difference between these two which is the absolute value right positive okay so this is the absolute value right positive example two minus y is absolute value is going to be plus two right so that's why you see absolute and it's called absolute there okay any question on this it is very common for example suppose that here um we want to make a model which uh tells that what what could be the failure uh maybe the failure is something between zero to one hundred based on the age okay let's say you have a machine and you want to know that what is the percentage of failure for that machine for different machines we have the age of that machine the higher the age the chance of failure is all right that's why you see here it's almost kind of like a linear relationship okay so suppose that we uh see the model using for example linear regression okay now we have the predictions okay so prediction is the green part h10 this is the failure and the green one is the model prediction age 20 for example here failure is 30 which is this and the green part is the prediction so the only thing we need to know just we need to find the error between predictions Prediction and failure, which is 11, right? And this one is two, right? These differences, and maybe here is negative five, because prediction is missing from truth. And after that one, we find the absolute value of this error, and then take that, which is eight point. So mean absolute error of this model is eight point five. Maybe you can ask, eight point five is good? Why it's not close to zero? It depends on the context, okay? How big, you know, sometimes, for example, eight is good, but in some systems, eight is very big number, all right? So that depends on the use case you are using. But the lower, we know that the lower failure, you have two models. When you want to compare model one with model two, so you check the mean absolute error of model one, that one is eight point five. If there is another model, it's around six, that's better than this, okay? But how much is good? We don't know, we don't know, okay? What I mean, we don't know, is something like this. Suppose that you want to predict the price of houses. Okay, so, and for the, one of the properties, the predicted value is, I don't know, 500, let's say 500,000 to 24, it's a predicted value, but the price of property is 500,000, 510,000, okay? So how much difference? 14,000. It is not far, right? But when you look at the 14,000, it's very big number. But we know that, okay, the predicted is almost similar. Close each other, okay, it's 14,000. You know what I mean? But in some context, like 14,000 is very, very big number. That's why I mean that depends on the use case, you look at that, okay, I see this much error. Is it acceptable or not? If you want to compare two models, the model with the mean absolute and less mean absolute error is gonna be the big, it's pretty, right? Question. I have a question. In the overfitting model, the mean absolutely error is very low, right? It's going to be very low number. Yeah. Almost, yeah. Almost zero. Again, it depends on the- Yeah, it depends on the case, yeah. For example, again, if you have it for the property prices, maybe 1,000 is very good prediction. Yeah, okay. Sure, yeah. If it's almost zero, that means that it's ideal, but it carries it over. On the other hand, we have also, I have forgot information. We have also underfitting model. Underfitting means, what is the underfitting model? It's opposite of overfitting. That means that it's not good on train and test, okay? Overfitting is good on the train, not in the test. Underfit means that it's useless. It's not good in train and test. Okay, so mean absolute error, okay, so again, let me say this example, mean absolute error. 8.5, it means that the average of our prediction about the number of the machine failure is, are increased by 8.5 machine failures, okay? So the other thing, root mean square, I personally an obligation problem, most of the time. this. Okay. I like this one better than this. There are some reasons. We're going to discuss about that. But like this one, as I mentioned, model prediction minus the ground truth squared and then squared. Why we do the square root? Because we have a square root. Right? Then we do this to make it normalize. Yeah? Okay. Okay. Sometimes we call that one residual. Okay. This is the example of how we can calculate RSME. Yeah. SRME is RMSE. Okay. Yeah. RMSE is the naming for this. So we have RMSE and we have MSE. This is M. So we want to calculate. This one is not MSE. We want to calculate RMSE. So rather than absolute value we need the square of that one and then take that. So this one reaches 1.9 whereas this one tells 8. Okay. So y minus y hat squared. This one divided by this one and gets squared. Any questions? A simple math behind. Okay. So when we have 9.9 we can think about that one. We can read that one as the error of 9.9. Machine failure on average per observation. Per observation we have 9.9 error. Okay. We're 9.9 per week error. Okay? Okay. So here there's just a comparison between those. As you mentioned, like, the value of zero isn't the perfect fit. Okay. So, but, it's an indicator of showing that it might be here kind of like for a city. Okay. So there's a good story here. or maybe first of all like a question we have MAE why do we need RMSE and why so here there is one advantage of RMSE compared to the MAE so let's back to this formula what is the difference between Y and Y hat is bigger or not what's going to happen it's going to have like suppose that you have a big number then you make it square square of that it's going to get the bigger number in other words what's happening here so rather than first consider this one if the difference between Y and Y hat is bigger you don't do any square you don't penalize so but then Y minus Y hat is bigger and we are using this formula in fact we make square of that one so for bigger number this number is getting bigger so for a small number it's going to be ok it doesn't make too much difference but at the end what's happening is that it penalizes a few large areas so it's going to be more than a lot of small errors ok so in other words if you want your model to avoid large errors use RMS ok because this one you know again like it gives you more error for big gaps does it make sense what I mean like when you have Y minus Y hat squared if you have like one people speaking Y and Y hat or Y minus Y hat squared you have like one people speaking Y and Y hat squared and Y hat is bigger then when you square that one it's going to be more bigger right suppose that you have 100 and then you find the square square of that one power 2 ok it's going to be 10,000 ok but for small values it's going to be just square of that one like 2 square of 2 is going to be 4 so at the end when you divide that one by N ok so if you divide that one by N if you have more these values the average also is getting to be like a big number that's why like in RMSE if you have some large number and you want to penalize the model based on that one RMSE helps you to do that yeah you see that it penalizes larger areas so more severely uh severe ending ok it's a hard it's a very average uh large number the thing is that like suppose you have some numbers like this you have 100 1000 a square of 2 and 1 and you take the average like a square of that one it's going to be a big number and when you divide that one by 4 and take the square root its value is going to be bigger than 1000 100 plus 1000 plus this one divided by 2 ok especially if you have more uh more of this particular number this error is going to be uh let's say significant that's why like it it helps us to penalize the larger error uh partial ok so uh error metric that we have uh in the regression model is called r squared ok so what is r squared r squared is nothing more than sum of the squared error divided by the uh let's say the s which is sum of the squared error divided by y minus y bar so rather than so what is what is the difference between these two one is prediction uh this one is the prediction and this one is the exactly point yeah it's the like a ground truth ok and we have y bar y bar is nothing more than average so what i mean is here the difference between the prediction of the model and average average and the difference between ground truth and average ok average of y no average of y average of ground truth ok so sometimes rather than this you use the variance of mean minus variance of the uh let's say the predicted values divided by again variance of the both of them are same so here s s r means sum of the square of residual sum of the square error sum of the square of residual difference between the model prediction minus average of ground truth and s s t is the sum of the square of difference between actual ground truth and average does it show this this this difference in What does it show, this metric being a point of view? Intuitively, what does it show? And then, like, intuitively it shows that, like, suppose that we have white hats here and here, right? We can skip both of them, right? So what you're gonna have is prediction of the model divided by, it shows that. And I mean that skip that one means that it's thinking this way, okay? So in general, R squared shows that how many times is the prediction of the model compared to the crunches, okay? It's the ratio of model prediction over the crunches. Make sense? It's like, yeah, simply skip this field, shows that how much bigger is our, how much bigger is our prediction. Let me see that we have example here. Okay, I think this one is a good example. So, okay, okay, let's go with this one. I think this one is the same, right? We don't need to, so what we are going to do here, we want to calculate this, okay? So first, y minus y hat, we have that one here. So let's see, take the, squared off that one, y hat, okay, and then take the average, and here, rather than y hat, use the y bar. Okay, we have y, and then we can calculate the y bar, okay, and the difference between each y and the average, squared, in other words, arrow. Okay, so then when we have these two, we apply that one in square root, which equals 0.85. Okay, so when this is just exactly the same, it's going to be zero. Sorry, it's going to be, when we have a perfect regression line, with no arrow, it's going to be one, right? Because it shows that y and y hat are almost the same. Otherwise, they're going to be the same. So the variance of line is going to be zero, and then this is going to be like . I think we don't need to continue too much about this. If I want to wrap up this discussion, what we use in reality is, for each regression model, we calculate M-A-E, we know how to calculate it, we use R-M-S-E, and also we use the R-squared. Okay? So for R-squared, the lower value is there. All of them, the lower value is there. Okay? R-squared on one shows they're perfect. Okay, so we have another one, which is adjusted R-squared error, which is the adjusted version of this, R-squared. So, and this is the formula for that one. Just see here what we have is, R-squared, we need to calculate R-squared from the previous formula. A is the number of records that we have, and P is the number of writes that we have. Just apply on this and get the average. So this one doesn't use . And we have M-A-P-E as well, in absolute percentage error, and this one is the formula. Again, I'm just reviewing this one, but we don't need to have, we don't need to focus on that one too much. So this is another thing which is called M-A-P-E. Okay? So you have this option, maybe vital programming, and you want to find the error, you can choose either of this. What is used in practice, M-A-E, R-M-S-E, and R-squared are the ones that we use. Okay. So that's why, that's why we see here, in sum, we have M-A-E, R-M-S-E, and R-squared. Okay. Sorry, just to wrap up this part, about R-squared. So R-squared tells us the degree to which the model explains the variance in the data. In other words, it shows how much better it is just than the mean. Okay? So the value of one indicates the perfect fit because of the, you know, this calculation we showed. If the variance of one is going to be zero, then divide it together, it's going to be one. And, and the zero means that it's not better than mean. So, I need to correct this part about the R-squared. I said that all of them, the lower value is good. For M-A-E, for R-M-S-E, yes, but for R-squared, the good one is one. Okay? You need to consider this. The good one is one. When it's zero, it shows that it's almost taking the average of the values. Okay? So, okay, let's say the bigger number is there. It goes to one. Okay? Clear? So, I corrected this part. When I mentioned that it's all of them should be lower. This two is okay, but for this one, it should be one for perfect. When it's like a zero, it means that it's nothing. It's just a random model, just taking the mean. Any question? And we know that it is not almost the cases, right? Sometimes our, okay, so, let's summarize here. So, if you have two models, then I have one model by decision three, one model by neural network, one model by linear regression. You want to compare them in terms of the accuracy of the model. Okay? And all of them for one specific problem. Okay? Let's say for prediction of the prices or costs. Okay? So, which one is good? We apply, we train this model, train this model, and this model, and this model. For each model, we calculate this net. Okay? So, which one has the lower MAE, lower RMSE, and higher square? Now, what is it doing? Well, lower usually means like overfitting, right? To lower. We need to consider for both of them, right? For both train and test. Okay. For now, suppose that we don't, we are not thinking about overfitting. Okay. Yeah. But, you know, we have to consider for both of them, right? For both train and test. Yeah. But, again, like what we have, like if it's close to zero, yes, there is overfitting. But, suppose that, like, we want to compare to model. Okay? One is 100. The other one is 90. Okay? There's no overfitting here. But, we know that the second one is better. Why? Because it's error. Yeah. I mean, that, lower error, it doesn't mean always, there is overfitting. Yeah. Yeah. Yeah. Yeah. There is overfitting. Okay? Okay. So, on the other hand, if, a beautiful thing is about the classification problem. So, for the classification problem, the story is a little bit different. Okay? When you want to find the performance of the classifiers, we don't use RMSC. Or, we don't use MAT. Okay? So, we use the other matrix here, which I'm going to explain. Okay? So, this slide is nothing more than, it shows that, like, what you have, you have X and Y. Based on X and Y, you train your learner, for model. Okay? And then, for new, unseen data, you pass only X, to the model, and it can predict it. For training, you need to predict it. Okay? And then, for new, unseen data, you pass only X, to the model, and it can predict it. For training, you need both X and Y. But, for prediction, you only need, to skip it. Right? Yes? Okay. Just nothing more than that one. So, here, what it says, that train your model, let's say your problem is a classification, so, for training, you need both X and Y. But, for prediction, you see that, the, the created model, dot, predict X, it tells you that, the, the model, okay, you don't pass those X and Y. Because if you pass those X and Y, why they are doing the prediction then, if you know the Y? Okay. So, again, this slide is, kind of like, repetitive. So, what performance metrics are important, because otherwise we can't, we can't, we don't know that the model performs well or not. So, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, I want to explain what is this matrix. Recall what we call this matrix. Yeah, we call that a confusion matrix. So, we call this one as confusion matrix. Okay? So, but it's not like it makes you confused. It's just named. Okay? So, what is that? So, suppose that you have a model, you train a model for classification to class A and class B. Okay? And, like, risky, non-risky. Class A is risky. Class B is non-risky. Okay? So, you have 1,000 samples. And for each sample, you know the ground truth. One is positive. Zero means negative. Okay? So, or another example is that we have 1,000 samples like for different people and each person can be patient or not. Okay? Patient or not. Or, this email is spam or not. Okay? It's 1 and 0. So, it's a classification. So, let's focus on the first one. Risky and non-risky. Some of the samples here are risky. Some of the samples are risky. Not risky. Let's take the risky one, which is the positive. So, ground truth is positive. The model prediction is also positive, is that here is 1, and the ground truth is also 1. Okay? So, what we call that, it is true positive. True positive. Okay? The type of positive which is correct. Okay? But, suppose that the model prediction is 1, but the ground truth is 0. In other words, the model tells that the sample is risky, but in reality, it is not risky. So, what is that? False positive. Okay? It's false positive. Okay? So, we have true positive and false positive. Let's think about the opposite of this. Okay? So, the opposite of this is like this. In ground truth, that sample is 0. Okay? It is non-risky in ground truth. The model prediction is also 0. It is called true negative. The type of negative which is true. But, if the ground truth is positive, the model tells that that's negative. It's a fake negative. Okay? In other words, it's a false negative. Make sense? So, that's why in this table, like, we have four cells here. Okay? So, the predicted class can be yes or no, and the actual class of ground truth also can be yes and no. So, this one is true positive. Okay? Both of them yes. But, this one is false positive. Because the prediction is positive. But, in reality, it is not a positive. So, it's a fake positive. And, same thing. Like a predicted model predicts that sample as the no, but in reality that's yes. So, it's a false negative. And true. Okay? Any questions on this? So far? So, which part is error? This one or this one? The fake one, right? False negative and false positive are errors. So, if I want to find the error of the model, it's going to be this one plus this one divided by all the samples. I mean that suppose that you have 100 samples. Out of that, here you have 30, you have 30 here, you have 20 here, or 25 here, and 15 here. Error is going to be, 25 plus 15 divided by all, which is 40%. Okay? Error is going to be 40%, but accuracy means that A plus P, the true positive, divided by all. Alright? In our case, it's going to be 60. Any questions? So, simply, accuracy means that the true positive and false. Negative parts divided by all the samples. So, under this, if this one is 60%, 1 minus that, accuracy is going to be Let me see that. Do I have an example? Okay. Let's take this one. Okay. So, suppose that the model prediction, the label, we have 9 samples, which is, the label is positive. But the model tells that, and the model tells that that's positive. Okay? It is not. What does this do? In reality, they are positive, but the model tells that they are negative. So, it's false, negative. So, this one, in reality, the model prediction is, the model prediction is positive, negative, but in reality that's positive so it's going to be also negative and we have eight items so what is the accuracy then for this model how many samples do we have 20 20. how many of those are correct classified this part 17 yeah and this part 17 over 20 which is 85 percent right iq ratio model is 85 and 15 percent is here okay i discussed about one of these uh metrics right and that is called just explaining to positive to negative so the accuracy is interpolated okay so what kind of issues do we have with accuracy like you see that accuracy like itself is a good indicator what is it depends on the size of the data set it's small it's small that depends on the size of data okay okay let me ask you this if we have accuracy of 95 and the size of data is bigger is it good like suppose i have one million sample and the accuracy of the model is uh 95 what does it mean it means that on 90 900 50 samples i can predict the models is good people for example we don't have a certain proof death sentence Yeah, exactly. Let's make it rather than 1 million, let's make it 100. So suppose that you have 100 people, okay? And the idea here is that we want to detect that. That person has specific cancer, okay? So if our model predicts, like, a 95% predict correctly, it means that only five of those is not correct. Sometimes, for example, the person is healthy, so that she has kind of cancer, and vice versa. The first one is okay. Like, if somebody is healthy, and the first one is healthy, and the second one is healthy, and vice versa. So let's tell you that, okay, you have cancer, maybe just mental issues, all right? But the second case is not tolerable, because the model cannot detect that one way. So you know what I mean? So I just want to mention that here, 91, 95%, or even 98% is not good. It means that out of 1 million people, let's say, 95%, 180K classified correctly, but 20,000 is not classified correctly, okay? So what I want to mention here is that accuracy itself, even if it's 98%, is not okay metric. It's misleading metric, okay? Itself is misleading. What we want to know, then, is that we can think about accuracy as one metric. We need to check that. On the other hand, we need to check two things. How many positive case I have in my ground truth? And how many of those I can detect? This one is important, right? There are five case, or let's say, 10 case with positive, and the model can only detect two of those. I can detect eight of those, right? So here, it's two out of 10. You know what I mean? This metric is called recall. So recall means that how many positives do you have in ground truth, and you are able to detect how many of those, correct? Okay? So of course, for the case of cancer, there are 10 people, I need to detect not 12, I need to detect higher value, maybe eight, maybe nine. That's this metric for me, okay? So this is called recall. So off of positive, how many of those I can detect first? Right? So maybe somebody comes out to pay. The simple way is that if you see something, just label that one as positive, all right? Then you will not find positive. Then you will not have any issue. True. Then what would be the issue with that? Recall is 100 here. Recall is getting 100. But accuracy of the model is dropping. True? Accuracy means that like it won't go in the same way. True. Okay. Let's go on the same example. Consider you have two classes. It's binary. And number of zero samples is 9,990. And there are 10 positives. So the model accuracy is 99.9%. If you look at this, you can see that the error is just 1 minus this, which is only 0.1. It is 1 minus 100 minus this. 1 is going to be 0. So this one is misleading. What is the alternative? We have recalls and we have precedents. Okay. If we're going through detail of this in terms of formula for this, just think about recalls, how many positive cases you have. And out of that one, how many of those are, how many of those are true positives? And the precision is that out of the predicted positives, how many of those are truly or correctly positive? What I mean is that, like, don't think about the formula like for now. Just simply think about that. How many samples do you have? How many of those are positive? 10. How many of 10s I can detect? Two. So recall is two. Two over 10. So I can detect four of those. It's going to be four of them. It's called recall. Precision means that, okay, among those ones that you labeled as positive, how many of those in reality are positive? Okay. For example, for example, I know that I have 20 samples as positive. 20 samples. As my model tells that there are 20 samples as positive. But in reality, I have only 10 positive samples. So precision here is that 10 divided by 20, which is 0.5. Okay? So that's why what we have here is recall means that among all the positives, how many of those are detected by me and by the model? And how many of those detected by the model are? And out of these positive cases by the model, how many of those are truly positive? Let me see that. Do we have an example? Okay. Let's think about this. I have 20 cases here. Okay? True positive is 9. True negative is 8. False positive, false negative. Accuracy is 0.8. Because accuracy just means that 9 plus 8 divided by 1. Okay? Let's look at the recall first. So how many positives do I have in general in total? Okay. 9 plus 1. Because this column is positive. I have 10 samples. And how many of those are detected by the model? Nine. Nine. So there will be 9 divided by . This is recall. Okay. Precision. So how many predicted positive we have in the model, in the model? 11, right? Model tells that 9 of those are positive. Positive. Positive. Here. Here. Here. Here. Here. Here. Here. Here. Here. Here. Here. Here. Here. Here. Here. Here. Here. And in reality, they are positive. And 12 of those are positive, but they are not. Okay? So it's going to be 9 divided by 11. 9 by 11 is getting to be 81, right? 9 divided by 11. Yeah. Okay. So the model tells that they are 11 positive case. But we know that 11 positive case, right? Out of that one, 9 of those are correct positive. 12 of those are not. 12 of those are fake positive. That's why, like, we say that simply. Precision means that this one divided by some of this gold. Recall means that this one divided by some of this gold. So that's why, like, 9 divided by 11. This precision, recall means that 9 divided by 10, which is 9. Memorize this. Maybe you can memorize it. But if I were you, I would just understand it constantly. What is the recall? Recall means that, again, how many positive you have in your ground truth, 10. How many of those I can detect? 9 of them. Okay? It's 90% recall. What is the precision? Precision means that among those new labels, model, new labels as the positive, how many of those in reality are positive, right? So it's going to be 9 of them. And I'm curious to know that those are correct labels. This one and this one. So I'm curious to know. This plus this divided by total. Recall means that this divided by this column. And precision means this one divided by. Which one I need to use? You need to record all these three varieties. Okay? You need to record all these three varieties. These three. Because accuracy itself is misleading. Yeah. Okay. I'm sorry. History. Accuracy, precision, and recall in the next slide. So I look at this. It's okay. But it's not the best. Let's check the precision and the recall. Okay. So a question. You know that accuracy is metric, but it's not everything. Among recall and precision, which one do you prefer? It depends on their use case. Okay. I'm asking this question and let me know recall is important or precision. Okay? So patient case. A person is patient or not. We need to what? We need to correct the positives. We don't miss the positive ones. That's the story. All right? We don't miss any positive here. When it's a use case like that, that means that recall is important. For example, I refer to the bank. Okay? Bank is a little bit flexible. But suppose that bank wants to not have any risky customer. Zero risky customer. If there's a risky customer, it's going to be, I don't know, like damage too much to the bank system. All right? Bank wants to detect every. That means that all the positive, among all the positive case, I need to detect all of them. That's the recall important. Same as patient. You want to see that all the cancer should be detected. Because it's not tolerable to label someone, which is one, as zero. Yeah. Okay? Okay. Let's say about spam. Okay? Spam. Spam, not spam. In your Gmail, in your Outlook, you get some emails, and the Outlook or Gmail automatically click that one too. Spam, not spam. Right? What is important? Important is that, like, it is not a spam, but you spam it. Right? It's not tolerable. So, it is important email, it is not spam, it should be in your inbox, but you spam it. So, detecting spams is not important. Among those you consider as spams, how many of those are, in reality, they are spams. This one is important. Okay? So, that's why we say precision is important. This is the case. Okay. I think this table is, okay. So, percentile is medical diagnosis. Okay? So, because missing a positive case is dangerous, so recall is important. Spam. You don't want to mark a real email as a spam. That means that, okay, if you label some email as a spam, but you are not, it's an issue. So, precision is important. Okay? So, in cyber security, there's some effects. Okay? And then you don't want to miss any attack. Okay? You don't want to miss any attack. Same as medical diagnosis. That's why recall is important. Of course, precision is also important. It's some weight. Because if... There is one, I don't know, like, but it is not attack, so you don't want to label it. Right? But the important thing is that if there is an attack, you need to detect it. So, that's why recall is important. Okay? Fraud detection, both is important. In the banking system, like, I mentioned in banking system, first thing is that, like, you don't want to... You don't want to miss any fraud. Right? That's why the recall is important. On the other hand, you don't want to label someone which is correct or not risky as a risky, because it's like it leads to that customer leave the company. Right? So, that's why in the fraud in the banking system, the first thing is that you don't want to miss any fraud. The first thing is that, first of all, risky people should not be missed. That's the first thing, because those are dangerous. On the other hand, if there is a good customer, you don't want to label as a bad customer, right? If you label it, maybe that customer may be, maybe that customer is going to leave. But if there is a risky customer, and you say, okay, of course, it's going to be 100% risky. So that's why, like, in cloud detection, like the bank system, insurance system, the first thing is recall. The second thing is also precision, okay? That's why, like we say, ghost. Medical diagnosis, never, right? Just the recall is important, okay? And again, here, the most important thing is recall. You don't want to miss any positive case. Any questions? Can you make another example from where the recall is important? A scenario that we don't want to miss any positive case. For example, autonomous cars. The best way is to ask the person to come to the car. Okay. Yeah, like... Are you talking autonomous driver? Yeah, yeah. Yeah, like autonomous driver. Yeah, like if... If you take the crossroads, okay? That's a tough line. It's a tough line, right? So you shouldn't miss that one. Autonomous driver shouldn't miss that one. If you missed, like, only one of those, maybe there's going to be a big crash, right? So precision is, again, like... Reepal is also important. Reepal is important there because if it is not crossroads and then you stop, it's going to be, I mean, chance of accident as well, right? But maybe it's controllable, right? So you just stop. But if it's stopping the crossroads and you pass it, of course, it's going to be accident, right? So, I mean, in some cases, both of them is important, but there's a wait. There is a wait. There is a wait. There is a wait. There is a wait. There is a wait for one of those. In a simple way, how much you are tolerating to the positive errors, okay? You can't miss, in your scenario, you can't miss any positive, so it's a reepal. By the way, okay, you see that pH here? Nobody asked me what is the pH. Who knows what is the pH? Special for what? So to explain that... I need to give some context. If you have a model for classification problems, the output of models can be 0 and 1, right? 0 and 1. But they don't generate exactly 0 and 1. They generate something between 0 and 1. Okay, like a probability. And we tell that, okay, it's higher than 0.5. We need to consider that as 1. If it's less than 0... 0.5, we need to consider as... 0. 0, negative. Okay? That's why, like, in this case, it says that, like, suppose that the threshold is 0.5 here. The model output, which could be probability, right? All the cases which is less than 0.5 is going to be negative. All the cases greater than 0.5. It's going to be positive. These green ones are truly positive. But we see that these two is different colors. You see? Like, there are two cases here. The color is not green. Okay? Suppose that green is positive. Okay? And white is negative. So what you see here is that there are two cases. Grown truths. They are white. But the model... The model prediction is about 0.5. That means that there are two labels as positive, but in reality, they are negative. That's right. On the other hand, you have eight items here, which are white, which are white, and also there are less than 0.5. So there are two positives, right? I have one case here. It is green. That means that it should be here by the model. But the model tells us its score is less than 0.5. It said false, negative. Right? This one is false, negative. This two is false, positive. This is true, positive. This is true, negative. So this threshold is just used to... To classify. So what is important after that, how many we have here, how many we have here, and... Okay. So, I have model A on the same problem, I have model B in the same problem, right? So one normal network here, I have, let's say, the linear regression. Okay. Now, linear regression, we call that normativity classification. We call it logistic regression. So we have two models. One logistic regression, one normal network on the same classification problem. And at the end, somebody told me that, which one do you prefer? The first thing is that I check their accuracy, recall the precision. If both of them have the same accuracy, accuracy majorly belongs to... Okay. It shouldn't be... Okay. US value, but the higher, it doesn't mean that it's always good. But it should be good accuracy, let's say 85, 90% for both of them. If this is the case, we look at the recall and precision. Depends on the use case, we check that which one is important for us. For the recall, so again, check in which one the recall is higher. Sometimes it's a little bit confusing. Let's say rather than recall and precision, I want to have one metric. I can take the average on this. I want to have both recall and precision higher. So I can take the average. And that is called F1 score. Because precision and recall, they generally... It's sort of like confidence. Because increasing the recall means that you want to catch more positives. And when you catch more positive, maybe you reduce precision. Because you're going to have a false output. So all those are not positive, but you label them as positive because you just want to detect all the positive. And you label some of those as X-positive. You detected all the positives, so your recall is higher. But on the other hand, you label some of those which are non-positive. And you wrongly label those as positive. So that's why recall and precision sometimes be complete. Or let's say often complete. So what we do, we want to get another metric, which is called F1 score. This one is very important. And it is just the average of these two. But it is not the normal average. Something like this one plus this one divided by two. It is harmonic average. And harmonic average means that multiplication of these two divided by summation. So precision times recall divided by summation. And then times two. In our case here, what is the recall? 0.8. What is the precision? It's 0.85. So F1 is going to be 0.9 times this, divided by 1.71, the summation. Times two. And then all of this. This one. Which returns something, if I calculate. . If I calculate that one. So yeah. We have precision. We have recall. If you take the average using this, harmonic average or harmonic gain, which means multiplication divided by summation times two. It's going to be kind of like average, right? Average is 85. And this one is . Like a normal average. So sometimes, rather than this two, we focus, OK, how much is . If this one is higher, it shows the recall and precision . OK. This one is good. OK. So we have recall 0.5. What does it mean? Only 50% of positive cases are detected by the model, right? 50% of actual patients were detected. Precision of 0.8 means that 80% of those detected as the positive were correct. So then F1 means, again, multiplication divided by . It should be summation. So I believe the final result is . It shows a moderate . And better than either metric or not. Any question? . Why we do the . Why we do the . What we're doing is it shows our sneezing. OK? . . ? . OK . Yeah . . . . . . . OK? . . . . . . . . . . . . . So you know, F1 is 0.5 is going to be 0.02. In other words, what we do, we penalize imbalance. If it's too imbalanced, this number should be smaller. But then if it's too imbalanced, you see that here, average is the same. Emphasis on the consistency. So it shows that both of them. This one shows that, OK, it's 0.2. And 0.2 means that even if precision is higher, but decal is lower, the combined is not good. One of the precision or decal is very low, then F1 drops sharply, like this example. OK, we can continue from the rest. I have two tree slides here, maybe two slides. For this one. I need to explain one. It's later in the conference. Any questions? So if I want to summarize today's discussion, I talked about two sort of models, regression models and classification. For a regression model, if you want to know model is good or not, just find the error. It goes between targets and . It could be mean absolute, or it could be . But you can apply same thing on classification problems. Why? Because if you apply mean absolute error, what's going to happen for classification problems? In classification, we have labels 0 and 1. Right? 0 and 1. If you take the average, that then gives you the float number. That's why . For classification, you need to find, rather than RMSE, you need to focus other things. Precision. How much we classify correctly. How much of the positive we classify correctly. This is called recall. The first one is called accuracy of the total accuracy of the model. That could be the misleading. That's why we go to recall. And precision means that out of the positive cases, you detect them. You detect them. In reality, how many of those were . So that's why we need to have all these three. Sometimes we tell that rather than recall and precision, let's define F1. So if you have precision, if you have accuracy and F1, you can sum up this one. It is very important and practical topic. And also like in, again, in exam, in interview, you're going to be asked about this. One of the common question is that, suppose you have a classifier. How do you know that it's good? You need to tell . Suppose that you are in bank system. You go to the interview for bank. So because they want to know that you understand the topic or not, say, OK, in this scenario, which metric do you think is good? OK. You're going to insurance. The insurance even asks you that which metric is something that you need to . It's very practical and it's used a lot. Yeah. Wherever you see that there is a classification, like risky, non-risky, most of the use case is something like this. It's a classifier to think about . Not just risky, non-risky. Different classes, but you're focusing for now on the . That can be . OK. So take a break. Maybe eight. Yeah. Is it ? It's going to be on the paper. OK. Yeah. There is three questions. two MCQ and one calculation. OK. Thank you. Multiple choice. Two more people choice. ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? . ? ? ? ? ? ? ? ? ? ? . ? ? ? It's basically the same as before, but it's a little different. It's a little different. It's a little different. It's a historical test. Did you take the test before? Do you want me to give you an A.I. award? Wow. Can you give me a hug? Do you want to hug me? I don't want to hug you. It's not necessarily the same. Because it used to be just two lines. Two lines of technology. It used to be two lines of technology. It's a little different. Because it used to be just two lines of technology. I don't know. It's a little different. The more you plan it, the more directions you want to take now and then. It's a little different. It's not necessarily all and all over again. Yeah. You don't always change the You don't always change the You don't always change the medium. medium. So I'm going to start with this You couldn't see me me I think it will be more difficult if you look at it, if you don't see it, you will think it is easy, but if you look at it, you will think it is very difficult, why is it so stupid? I think it will be more difficult if you look at it, if you don't see it, you will think it is easy, but if you don't see it, you will think it is very difficult, why is it so stupid? I don't know how to read the screen, so I choose this one. Yes, it really is. It has to be divided into two parts. I have to cough twice a day. I have to cough twice a day. I have to cough twice a day. I have to cough twice a day. I have to cough twice a day. It's fun. I was thinking, why did I write it all to math? I have to escape! I have to eat! I have to help the horse. We'd better look at it, because I'm hungry! We'd better look at it, because I'm hungry! I look at it with thesenn can. I look at it with thesenn can. I didn't learn fuzzing. If you didn't learn it, there's nothing you can do. I'm looking at the choices right now. You don't have time, right? If I didn't learn it, I wouldn't take the exam. Take the exam. You learned it in the first class. Fuzzing? I learned it in the first class. You really learned it? I saw it in the first class in the classroom. I'm the same as you. I didn't see that question. I really didn't. But he mentioned it. Where did you mention it? He mentioned it on social media. Did you learn fuzzing? I understand. But I'm looking at the choices right now. He said it. He said two choices. Go take a look. I know. Like in this class. I saw that in the question. He said it in this class. Yeah, that's possible. You took the exam. You learned it in the first class. Then you took the exam in the second class. I want to see it first. I don't know. I admitted that I took the exam. You didn't take the exam. It's okay to take the exam later. You can take the exam at the end. But you can take the exam after you memorize the formula. You can take it after you memorize the formula. So take it. Anyway, when I asked him, he said he would take the exam, but there was no limit to the topic. He said he would take the exam, but there was no limit to the topic. He said he would take the exam, but there was no limit to the topic. He said he would take the exam, but there was no limit to the topic. He said he would take the exam, but there was no limit to the topic. He said he would take the exam, but there was no limit to the topic.